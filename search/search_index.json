{"config":{"lang":["en"],"separator":"[\\s\\-\\.]","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Acturial Capital","text":""},{"location":"--/example/","title":"Example","text":"<pre><code>graph LR\n  A[Start] --&gt; B{Error?};\n  B --&gt;|Yes| C[Hmm...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Yay!];</code></pre> \\[ \\operatorname{ker} f=\\{g\\in G:f(g)=e_{H}\\}{\\mbox{.}} \\] <p>The homomorphism \\(f\\) is injective if and only if its kernel is only the  singleton set \\(e_G\\), because otherwise \\(\\exists a,b\\in G\\) with \\(a\\neq b\\) such  that \\(f(a)=f(b)\\).</p> <pre><code>theme:\nfeatures:\n- content.code.annotate # (1)\n</code></pre> <ol> <li> I'm a code annotation! I can contain <code>code</code>, formatted     text, images, ... basically anything that can be written in Markdown.</li> </ol> <p>The <code>range()</code> function is used to generate a sequence of numbers.</p> Tab 1Tab 2 <p>Lorem ipsum dolor sit amet, (1) consectetur adipiscing elit.</p> <ol> <li> I'm an annotation!</li> </ol> <p>Phasellus posuere in sem ut cursus (1)</p> <ol> <li> I'm an annotation as well!</li> </ol>"},{"location":"advanced_signals/","title":"Transformed Dataset","text":""},{"location":"advanced_signals/nowcast/","title":"Nowcast","text":""},{"location":"api/","title":"API Reference","text":"<p>Strategy is the main implementation of the library for portfolio construction, which initializes the execution of the sequence of blocks and allows to compute tilts/exposures, create and backtest target portfolios.</p> <p></p> <p>Signals constructs transfomed dataset, which are used to build strategies. Examples including High Frequency Indicators, Nowcasts/Forecasts or Alpha Signals.</p> <p></p> <p>Categorize, which is the process of creating bucket of investables that aligns with an market risk on and off, cyclicality, factor importance and general benchmark behaviour.</p>"},{"location":"api/categorize/","title":"Categorize","text":""},{"location":"api/categorize/#coming-soon","title":"Coming Soon!","text":""},{"location":"api/signals/","title":"Signals","text":""},{"location":"api/signals/#coming-soon","title":"Coming Soon!","text":""},{"location":"api/strategy/","title":"Strategy","text":""},{"location":"api/strategy/#strategy_1","title":"Strategy","text":"<pre><code>opendesk.Strategy(\nsteps: List[Tuple[str, Type, Dict, Dict]], \ntopdown: Optional[bool] = False, \nmapping_table: Optional[Dict[str, str]] = None, \nmapping_weights: Optional[Dict[int, Tuple]] = None\n)\n</code></pre> <p>The main implementation of the library is the <code>Strategy</code> class, which initializes the execution of the sequence and allows to compute tilts/exposures, create and backtest portfolios.</p> <p>Fluent interface</p> <p>In object-oriented programming, returning self from a method allows for the implementation of a fluent interface, where methods can be cascaded i a single statement.</p> <p>In object-oriented programming, returning <code>self</code> from a method can be useful for several reasons. One common use case is to create a fluent interface, which is an API design style that allows method calls to be chained together in a single statement. This can make the code more readable and concise, as it eliminates the need to create intermediate variables to store the results of intermediate method calls. For example, with a fluent interface, this code could be written as <code>results = SomeClass().method1().method2().method3()</code>.</p>"},{"location":"api/strategy/#parameters","title":"Parameters","text":"steps<pre><code>List[Tuple[str, Type, Dict, Dict]]\n</code></pre> <p>Alpha blocks in order of execution. More information, please visit the Model Glossary. The parameter <code>steps</code> is required to enable the smooth integration of the strategy into the module with minimal disruption. It is a list of tuples containing:</p> <ul> <li>Custom name of the selected block (e.i. <code>myblock</code>)</li> <li>Class object of the selected block (e.i. <code>MyBlock</code>)</li> <li>Necessary parameters used at the block initialization (<code>__init__</code>) section (e.i. <code>dict(param_1 = \"param_1, param_2 = \"param_2\")</code>)</li> <li>Additional data, <code>**kwargs</code>, required to train the model from the method <code>processing()</code></li> </ul> <p>Example</p> <pre><code>from opendesk.blocks import Reversion\nsteps=[(\"Reversion\", Reversion)]\n</code></pre> topdown<pre><code>Optional[bool] = False\n</code></pre> <p>Activates top-down strategy. The strategy tilts is processed at a higher level (e.i. sector level) than the actual allocation exectution (e.i. stock level). If set to <code>True</code>, a mapping table should be passed. Defaults to <code>False</code>.</p> mapping_table<pre><code>Optional[Dict[str, str]] = None\n</code></pre> <p>Maps higher with lower level assets. Variable <code>topdown</code> needs to be turned to <code>True</code>. Defaults to <code>None</code>.</p> mapping_weights<pre><code>Optional[Dict[int, Tuple]] = None\n</code></pre> <p>Maps scores with range of weights. Defaults to <code>None</code>.</p>"},{"location":"api/strategy/#ancestors-in-mro","title":"Ancestors (in MRO)","text":"<ul> <li>opendesk.portfolio.PortfolioConstruction</li> </ul>"},{"location":"api/strategy/#attributes","title":"Attributes","text":""},{"location":"api/strategy/#breakdown","title":"breakdown","text":"breakdown<pre><code>pandas.core.frame.DataFrame\n</code></pre> <p>Output (scores) of all provided blocks. Following the method <code>fit()</code>, the attribute <code>breakdown</code> can be accessed, which contains the output from various models in a single <code>pandas.DataFrame</code> object</p> <p>Breakdown</p> <p> <pre><code>$ strategy.breakdown\n&lt;span style=\"color: grey;\"&gt;           Model 1  Model 2  Model 3  Model 4\nsector 1        -1        0        2        1\nsector 2         2       -2        0        1\nsector 3         1        1       -2        2\nsector 4        -2        0       -1        0\nsector 5         0       -1       -1        2\nsector 6        -1        0        1       -2\nsector 7        -2        1       -2       -1\nsector 8         1        2        0       -1\nsector 9         0        0       -1        0\nsector 10        2        0        1        0\n&lt;/span&gt;\n</code></pre> </p>"},{"location":"api/strategy/#exposures","title":"exposures","text":"exposures<pre><code>pandas.core.frame.DataFrame\n</code></pre> <p>Strategy exposures/tilts aggregated from model scores.</p>"},{"location":"api/strategy/#model_data","title":"model_data","text":"model_data<pre><code>pandas.core.frame.DataFrame\n</code></pre> <p>Adjusted closing prices of the asset, each row is a date and each column is a ticker/id.</p>"},{"location":"api/strategy/#weights","title":"weights","text":"weights<pre><code>Dict[str, float]\n</code></pre> <p>Portfolio weights calculated through the discrete allocation <code>method</code>.</p> <p>Bounds</p> Group ConstraintsLower BoundMid BoundUpper Bound <p> <pre><code>$ strategy.group_constraints\n&lt;span style=\"color: grey;\"&gt;{'sector 1': (0.0, 0.0),\n'sector 2': (0.05, 0.15),\n'sector 3': (0.0, 0.0),\n'sector 4': (-0.15, -0.05),\n'sector 5': (0.0, 0.0),\n'sector 6': (0.0, 0.0),\n'sector 7': (0.0, 0.0),\n'sector 8': (0.05, 0.15),\n'sector 9': (0.0, 0.0),\n'sector 10': (0.0, 0.0)}\n&lt;/span&gt;\n</code></pre> </p> <p> <pre><code>$ strategy.lower_bound\n&lt;span style=\"color: grey;\"&gt;{'sector 1': 0.0,\n'sector 2': 0.05,\n'sector 3': 0.0,\n'sector 4': -0.15,\n'sector 5': 0.0,\n'sector 6': 0.0,\n'sector 7': 0.0,\n'sector 8': 0.05,\n'sector 9': 0.0,\n'sector 10': 0.0}\n&lt;/span&gt;\n</code></pre> </p> <p> <pre><code>$ strategy.mid_bound\n&lt;span style=\"color: grey;\"&gt;{'sector 1': 0.0,\n'sector 2': 0.1,\n'sector 3': 0.0,\n'sector 4': -0.1,\n'sector 5': 0.0,\n'sector 6': 0.0,\n'sector 7': 0.0,\n'sector 8': 0.1,\n'sector 9': 0.0,\n'sector 10': 0.0}\n&lt;/span&gt;\n</code></pre> </p> <p> <pre><code>$ strategy.upper_bound\n&lt;span style=\"color: grey;\"&gt;{'feature 1': 0.0,\n'feature 2': 0.15,\n'feature 3': 0.0,\n'feature 4': -0.05,\n'feature 5': 0.0,\n'feature 6': 0.0,\n'feature 7': 0.0,\n'feature 8': 0.15,\n'feature 9': 0.0,\n'feature 10': 0.0}\n&lt;/span&gt;\n</code></pre> </p>"},{"location":"api/strategy/#instance-variables","title":"Instance variables","text":""},{"location":"api/strategy/#group_constraints","title":"group_constraints","text":"group_constraints<pre><code>Optional[Dict[str, Tuple(float, float)]]\n</code></pre> <p>Strategy constraints by group. Product of <code>exposures</code> and <code>mapping_weights</code>.</p>"},{"location":"api/strategy/#public-methods","title":"Public Methods","text":""},{"location":"api/strategy/#exposures_1","title":"Exposures","text":"<ul> <li><code>add_blocks()</code>: After initialization, additional blocks can be added to <code>steps</code></li> <li><code>check_group_constraints()</code>: Check group constraints after creating a portfolio</li> <li><code>estimate()</code>: Aggregate exposures by summing each units using a predetermined function</li> <li><code>fit()</code>: Executes each provided blocks with provided dataset</li> <li><code>run_block()</code>: Executes one block</li> </ul>"},{"location":"api/strategy/#portfolio-construction","title":"Portfolio Construction","text":"<ul> <li><code>portfolio()</code>: Find portfolio weights, at any levels</li> <li><code>discrete_allocation()</code>: Set portfolio weights following a discrete allocation weighting scheme</li> <li><code>optimize()</code>: Portfolio optimization, which aims to select the optimal mix of assets in a portfolio in order to satisfy the defined objectives and constraints</li> </ul>"},{"location":"api/strategy/#backtest","title":"Backtest","text":"<ul> <li><code>backtest()</code>: Backtest portfolio</li> </ul>"},{"location":"api/strategy/backtest/","title":"Backtest","text":""},{"location":"api/strategy/backtest/#backtest_1","title":"backtest","text":"<pre><code>@classmethod\nopendesk.Strategy.backtest(\nconfig: BacktestConfig, \n**kwargs\n) \u2011&gt; vectorbt.portfolio.base.Portfolio\n</code></pre> <p>The <code>backtest</code> instance of the <code>Strategy</code> class is a classmethod. It is used to run a simulation using a custom order function <code>from_order_func</code> from <code>vectorbt.Portfolio</code> class. </p> <p>The job of the Portfolio class is to create a series of positions allocated against a cash component, produce an equity curve, incorporate basic transaction costs and produce a set of statistics about its performance. In particular, it outputs position/profit metrics and drawdown information.</p>"},{"location":"api/strategy/backtest/#workflow","title":"Workflow","text":""},{"location":"api/strategy/backtest/#preparation","title":"Preparation","text":"<ul> <li>Receives a set of inputs, such as signal arrays and other parameters</li> <li>Resolves parameter defaults by searching for them in the global settings</li> <li>Brings input arrays to a single shape</li> <li>Does some basic validation of inputs and converts Pandas objects to NumPy arrays</li> <li>Passes everything to a Numba-compiled simulation function</li> </ul>"},{"location":"api/strategy/backtest/#simulation","title":"Simulation","text":"<ul> <li>The simulation function traverses the broadcasted shape element by element, row by row (time dimension), column by column (asset dimension)</li> <li>For each asset and timestamp (= element):<ul> <li>Gets all available information related to this element and executes the logic</li> <li>Generates an order or skips the element altogether</li> <li>If an order has been issued, processes the order and fills/ignores/rejects it</li> <li>If the order has been filled, registers the result by appending it to the order records</li> <li>Updates the current state such as the cash and asset balances</li> </ul> </li> </ul>"},{"location":"api/strategy/backtest/#construction","title":"Construction","text":"<ul> <li>Receives the returned order records and initializes a new Portfolio object</li> </ul>"},{"location":"api/strategy/backtest/#analysis","title":"Analysis","text":"<ul> <li>Offers a broad range of risk &amp; performance metrics based on order records</li> </ul> <p>It requires <code>BacktestConfig</code> dataclass, a configuration pipeline which simplifies model configurations.</p> <p><code>@classmethod</code></p> <p>A class method is a method that is bound to the class and not the instance of the class. It can be called on the class itself, as well as on any instance of the class. In Python, a class method is defined using the <code>@classmethod</code> decorator.</p> <p>The <code>backtest</code> classmethod initially initializes and fits the strategy using the <code>fit()</code> method, and estimates its exposures using the <code>estimate()</code> method. Afterwards, it constructs a portfolio based on specified orchestration and methodologies at each point in time. The final result is a vectorbt object, which provides access to the full range of functionality offered by the vectorbt ecosystem.</p>"},{"location":"api/strategy/backtest/#parameters","title":"Parameters","text":"config<pre><code>opendesk.backtest.config.BacktestConfig\n</code></pre> <p>Backtesting configuration pipeline <code>dataclass</code>, which includes the necessary parameters to activate the internal strategy builder, is required for this process.</p> kwargs<pre><code>Dict\n</code></pre> <p>Additional parameters for <code>vbt.portfolio.base.Portfolio.from_order_func</code> function. It accepts:</p> <ul> <li>wrapper: ArrayWrapper,</li> <li>close: tp.ArrayLike,</li> <li>order_records: tp.RecordArray,</li> <li>log_records: tp.RecordArray,</li> <li>init_cash: tp.ArrayLike,</li> <li>cash_sharing: bool,</li> <li>call_seq: tp.Optional[tp.Array2d] = None,</li> <li>fillna_close: tp.Optional[bool] = None,</li> <li>trades_type: tp.Optional[tp.Union[int, str]] = None) -&gt; None:</li> </ul> <p>When Vectorbt Default to None<p>If you look at the arguments of each class method, you will notice that most of them default to None. None has a special meaning in <code>vectorbt</code>: it's a command to pull the default value from the global settings config - <code>settings</code>. The branch for the Portfolio can be found` under the key 'portfolio'. For example, the default size is:</p> <pre><code>vbt.settings.portfolio['size']\n</code></pre> </p>"},{"location":"api/strategy/backtest/#returns","title":"Returns","text":"<p><code>vectorbt.Portfolio.from_order_func</code>: vectorbt <code>Portfolio</code> ecosystem.</p> <p>Example Backtest<p>The <code>backtest</code> classmethod is initalized through the <code>BacktestConfig</code> dataclass, which facilitates feature integration. Mandatory variables <code>universe</code>, <code>model_data</code> and <code>steps</code> are set, as follow:</p> <p> <pre><code>$ from opendesk import BacktestConfig\n$ from opendesk import Strategy\n$ config = BacktestConfig(\n$     universe=stock_prices, $     model_data=model_data, $     steps=steps\n$ )\n$ backtest = Strategy.backtest(config)\n$ backtest.stats()\n&lt;span style=\"color: grey;\"&gt;Start                                 2019-01-02 00:00:00\nEnd                                   2022-12-30 00:00:00\nPeriod                                 1008 days 00:00:00\nStart Value                                         100.0\nEnd Value                                      152.092993\nTotal Return [%]                                52.092993\nBenchmark Return [%]                            62.173178\nMax Gross Exposure [%]                          31.819902\nTotal Fees Paid                                       0.0\nMax Drawdown [%]                                13.822639\nMax Drawdown Duration                   320 days 00:00:00\nTotal Trades                                          391\nTotal Closed Trades                                   372\nTotal Open Trades                                      19\nOpen Trade PnL                                  14.614093\nWin Rate [%]                                    54.301075\nBest Trade [%]                                 122.866679\nWorst Trade [%]                               -164.309231\nAvg Winning Trade [%]                           22.115064\nAvg Losing Trade [%]                            -19.44764\nAvg Winning Trade Duration    158 days 21:51:40.990099010\nAvg Losing Trade Duration     143 days 02:49:24.705882354\nProfit Factor                                    1.417933\nExpectancy                                        0.10075\nSharpe Ratio                                     0.942305\nCalmar Ratio                                     0.799575\nOmega Ratio                                      1.179846\nSortino Ratio                                    1.431216\nName: group, dtype: object\n&lt;/span&gt;\n</code></pre> </p> </p>"},{"location":"api/strategy/backtest/#backtestconfig","title":"BacktestConfig","text":"<pre><code>@dataclass\nbacktest.config.BacktestConfig(\nsteps:\u00a0List, \nuniverse:\u00a0pandas.core.frame.DataFrame, \nmodel_data:\u00a0Optional[pandas.core.frame.DataFrame]\u00a0=\u00a0None, \ntopdown:\u00a0Optional[bool]\u00a0=\u00a0False, \nmapping_table:\u00a0Optional[Dict]\u00a0=\u00a0None, \nmapping_weights:\u00a0Optional[Dict]\u00a0=\u00a0None, \nfreq:\u00a0Optional[int]\u00a0=\u00a0252, \nverbose:\u00a0Optional[bool]\u00a0=\u00a0False, \nfit_backend:\u00a0Optional[str]\u00a0=\u00a0'joblib', \nestimate:\u00a0Optional[Type]\u00a0=\u00a0&lt;built-in function sum&gt;, \nportfolio:\u00a0Optional[str]\u00a0=\u00a0'optimize', \noptimize_model:\u00a0Optional[str]\u00a0=\u00a0'mvo', \noptimize_cov_matrix_params:\u00a0Optional[Dict[str,\u00a0Any]]\u00a0=\u00a0&lt;factory&gt;, \noptimize_expected_returns_params:\u00a0Optional[Dict[str,\u00a0Any]]\u00a0=\u00a0&lt;factory&gt;, \noptimize_black_litterman:\u00a0Optional[bool]\u00a0=\u00a0False,\noptimize_black_litterman_params:\u00a0Optional[Dict[str,\u00a0Any]]\u00a0=\u00a0None, \noptimize_weight_bounds:\u00a0Optional[Tuple[int,\u00a0int]]\u00a0=\u00a0(-1, 1), \noptimize_solver:\u00a0Optional[str]\u00a0=\u00a0None, \noptimize_verbose:\u00a0Optional[bool]\u00a0=\u00a0False, \noptimize_solver_options:\u00a0Optional[Dict]\u00a0=\u00a0None, \nadd_alpha_block_constraints:\u00a0Optional[bool]\u00a0=\u00a0True, \nadd_n_asset_constraints:\u00a0Optional[int]\u00a0=\u00a0None, \nadd_l2_regularization:\u00a0Optional[bool]\u00a0=\u00a0True, \nadd_gamma:\u00a0Optional[int]\u00a0=\u00a02, \nadd_custom_objectives:\u00a0Optional[List[Tuple[Type,\u00a0Dict[str,\u00a0Any]]]]\u00a0=\u00a0None, \nadd_custom_constraints:\u00a0Optional[List[Type]]\u00a0=\u00a0None, \noptimize_method:\u00a0Optional[str]\u00a0=\u00a0'efficient_risk', \noptimize_params:\u00a0Optional[Dict[str,\u00a0Any]]\u00a0=\u00a0&lt;factory&gt;, \ndiscrete_allocation_model:\u00a0Optional[str]\u00a0=\u00a0'equal_weighted',\ndiscrete_allocation_model_params:\u00a0Dict[str,\u00a0Any]\u00a0=\u00a0None, \ndiscrete_allocation_range_bound:\u00a0Optional[str]\u00a0=\u00a0'mid', \nbacktest_every_nth:\u00a0Optional[int]\u00a0=\u00a030, \nbacktest_history_len:\u00a0Optional[int]\u00a0=\u00a0-1, \nbacktest_direction:\u00a0Optional[str]\u00a0=\u00a0'long_short', \nbacktest_backup:\u00a0Optional[str]\u00a0=\u00a0'discrete_allocation', \nbacktest_backup_params:\u00a0Optional[Dict[str,\u00a0Any]]\u00a0=\u00a0&lt;factory&gt;\n)\n</code></pre> <p><code>@dataclass</code></p> <p>A <code>dataclass</code> is a decorator that is used to define a class that will be used to store data. It automatically generates special methods, such as <code>__init__</code>, <code>__repr__</code>, and <code>__eq__</code>, based on the class's attributes. A dataclass is defined using the <code>@dataclass</code> decorator.</p>"},{"location":"api/strategy/backtest/#parameters_1","title":"Parameters","text":"steps<pre><code>List[Tuple[str, Type, Dict, Dict]]\n</code></pre> <p>Alpha blocks in order of execution. More information, please visit the Model Glossary. The parameter <code>steps</code> is required to enable the smooth integration of the strategy into the module with minimal disruption. It is a list of tuples containing:</p> <ul> <li>Custom name of the selected block (e.i. <code>myblock</code>)</li> <li>Class object of the selected block (e.i. <code>MyBlock</code>)</li> <li>Necessary parameters used at the block initialization (<code>__init__</code>) section (e.i. <code>dict(param_1=\"param_1, param_2=\"param_2\")</code>)</li> <li>Additional data, <code>**kwargs</code>, required to train the model from the method <code>processing()</code></li> </ul> universe<pre><code>pandas.core.frame.DataFrame\n</code></pre> <p>Market price time-series universe, each row is a date and each column is a ticker/id.</p> model_data<pre><code>Optional[pandas.core.frame.DataFrame] = None\n</code></pre> <p>Market returns time-series used to train the model.</p> topdown<pre><code>Optional[bool] = False\n</code></pre> <p>Activates top-down strategy. The strategy tilts is processed at a higher level (e.i. sector level) than the actual allocation exectution (e.i. stock level). If set to <code>True</code>, a mapping table should be passed. Defaults to <code>False</code>.</p> mapping_table<pre><code>Optional[Dict[str, str]] = None\n</code></pre> <p>Maps higher with lower level assets. Variable <code>topdown</code> needs to be turned to <code>True</code>. Defaults to <code>None</code>.</p> mapping_weights<pre><code>Optional[Dict[int, Tuple]] = None\n</code></pre> <p>Maps scores with range of weights. Defaults to <code>None</code>.</p> fit_backend<pre><code>Optional[str] = \"joblib\"\n</code></pre> <p>Run parallel multiprocessing or iterative process.</p> verbose<pre><code>Optional[bool] = None\n</code></pre> <p>Progress messages are printed. Applied to <code>fit()</code> and <code>optimize()</code> functions.</p> estimate<pre><code>Optional[Type] = sum\n</code></pre> <p>Strategy exposures/tilts aggregated from model scores. The <code>func</code> parameter can be any object that is compatible with the <code>.apply</code> function in the pandas library.</p> portfolio<pre><code>Optional[str] = \"optimize\"\n</code></pre> <p>Portfolio construction procedure to allocate weights. It could be <code>optimize</code> or <code>discrete allocation</code>. Defauts to <code>optimize</code></p> optimize_model<pre><code>Optional[str] = \"mvo\"\n</code></pre> <p>Type of optimizer to be used. Type of optimization:</p> <ul> <li><code>mvo</code>: Mean-variance optimization</li> <li><code>hrp</code>: Hierarchical Risk Parity</li> </ul> optimize_expected_returns_params<pre><code>Optional[Dict[str, Any]]\n</code></pre> <p>Parameters to compute an estimate of future returns:</p> <ul> <li><code>method</code> (str): the return model to use. Should be one of:<ul> <li><code>mean_historical_return</code></li> <li><code>ema_historical_return</code></li> <li><code>capm_return</code></li> </ul> </li> <li><code>**kwargs</code>: Method specificities Defaults to:</li> </ul> <pre><code>dict(\nmethod=\"ema_historical_return\", \ncompounding=True, \nspan=500, \nfrequency=252, \nlog_returns=False\n)\n</code></pre> optimize_cov_matrix_params<pre><code>Optional[Dict[str, Any]]\n</code></pre> <p>Parameters to compute a covariance matrix:</p> <ul> <li><code>method</code> (str): the risk model to use. Should be one of:<ul> <li><code>sample_cov</code></li> <li><code>semicovariance</code></li> <li><code>exp_cov</code></li> <li><code>ledoit_wolf</code></li> <li><code>ledoit_wolf_constant_variance</code></li> <li><code>ledoit_wolf_single_factor</code></li> <li><code>ledoit_wolf_constant_correlation</code></li> <li><code>oracle_approximating</code></li> </ul> </li> <li><code>**kwargs</code>: Method specificities Defautls to:</li> </ul> <pre><code>dict(\nmethod=\"ledoit_wolf\", \nfrequency=252, \nlog_returns=False\n)\n</code></pre> optimize_weight_bounds<pre><code>Optional[Tuple[int, int]] = (-1, 1)\n</code></pre> <p>Minimum and maximum weight of each asset or single min/max pair if all identical, defaults to (-1, 1). If <code>weight_bounds=(-1, 1)</code>, allows short positions. Defaults to <code>(-1, 1)</code>.</p> optimize_solver<pre><code>Optional[str] = None\n</code></pre> <p>name of solver. list available solvers with: <code>cvxpy.installed_solvers()</code>.</p> optimize_solver_options<pre><code>Optional[Dict] = None\n</code></pre> <p>Parameters for the given solver.</p> add_alpha_block_constraints<pre><code>Optional[bool] = True\n</code></pre> <p>Alpha blocks core constraints. It adds constraints on the sum of weights of different groups of assets. Most commonly, these will be sector constraints. These constraints a particularly relevant when working with alpha blocks (top-down or bottom-up), as we aim to limit our exposure to paricular group of assets. Defaults to <code>True</code>.</p> add_n_asset_constraints<pre><code>Optional[List[Type]] = None\n</code></pre> <p>Number of assets in the portfolio constraints. Cardinality constraints are not convex, making them difficult to implement. However, we can treat it as a mixed-integer program and solve (provided you have access to a solver). for small problems with less than 1000 variables and constraints, you can use the community version of CPLEX available in python <code>pip install cplex</code>.</p> <p><code>n_asset_constraints</code></p> <p>This functionnality is still work in progress, as it requires external capabilities (<code>cplex</code>).</p> add_l2_regularization<pre><code>Optional[bool] = True\n</code></pre> <p>L2 regularisation, i.e \\(\\gamma ||w||^2\\), to increase the number of nonzero weights.</p> <p>Mean-variance optimization often results in many weights being negligible, i.e the efficient portfolio does not end up including most of the assets. This is expected behaviour, but it may be undesirable if you need a certain number of assets in your portfolio. </p> <p>In order to coerce the mean-variance optimizer to produce more non-negligible weights, we add what can be thought of as a \u201csmall weights penalty\u201d to all of the objective functions, parameterised by \\(\\gamma\\) (gamma). This term reduces the number of negligible weights, because it has a minimum value when all weights are equally distributed, and maximum value in the limiting case where the entire portfolio is allocated to one asset. We refer to it as L2 regularisation because it has exactly the same form as the L2 regularisation term in machine learning, though a slightly different purpose (in ML it is used to keep weights small while here it is used to make them larger).</p> <p>Gamma</p> <p>In practice, \\(\\gamma\\) must be tuned to achieve the level of regularisation that you want. However, if the universe of assets is small (less than 20 assets), then gamma=1 is a good starting point. For larger universes, or if you want more non-negligible weights in the final portfolio, increase gamma.</p> add_gamma<pre><code>Optional[int] = 2\n</code></pre> <p>L2 regularisation parameter. Defaults to 2. Increase if you want more non-negligible weights</p> add_custom_objectives<pre><code>Optional[List[Tuple[Type, Dict[str, Any]]]] = None\n</code></pre> <p>List of lambda functions to add new term into the based objective function. This term must be convex, and built from cvxpy atomic functions.</p> add_custom_constraints<pre><code>Optional[List[Type]] = None\n</code></pre> <p>List of lambda function (e.i. all assets &lt;= 3% of the total portfolio = [lambda w: w &lt;= .03]. This constraint must satisfy DCP rules, i.e be either a linear equality constraint or convex inequality constraint.</p> optimize_method<pre><code>Optional[str] = \"efficient_risk\"\n</code></pre> <p>Optimization method that can be called (corresponding to different objective functions).</p> <p>Object Instantiation</p> <p>A new EfficientFrontier object should be instantiated if you want to make any change to objectives/constraints/bounds/parameters.  The backtesting framework re-instantiate the optimization process at every rebalancing periods.</p> optimize_params<pre><code>Optional[Dict[str, Any]] = dict(target_volatility=0.08, market_neutral=True)\n</code></pre> <p>Optimization method parameters. Defaults to:</p> <pre><code>dict(\ntarget_volatility=0.08, \nmarket_neutral=True\n)\n</code></pre> discrete_allocation_method<pre><code>Optional[str] = \"uniform\"\n</code></pre> <p>Method used to allocate weights.</p> discrete_allocation_model_params<pre><code>Dict[str, Any] = None\n</code></pre> <p>Model specific parameters.</p> discrete_allocation_range_bound<pre><code>Optional[str] = \"mid\"\n</code></pre> <p>Bound from <code>mapping_weights</code>. Total budget (in %) to apply.</p> backtest_every_nth<pre><code>Optional[int] = 30\n</code></pre> <p>Backtest rebalancing period in days. Defaults to 30. </p> backtest_history_len<pre><code>Optional[int] = -1\n</code></pre> <p>Backtest model training period. If -1, the model trains the entire history. Defaults to -1.</p> backtest_direction<pre><code>Optional[str] = \"long_short\"\n</code></pre> <p>Backtest Direction. It can be <code>long_only</code>, <code>short_only' or</code>long_short<code>. Defaults to</code>long_short`.</p> backtest_backup<pre><code> Optional[str] = \"discrete_allocation\"\n</code></pre> <p>Backtest \"back-up\" used as a fallback in the event that the optimizer is unable to deliver feasible weights. It can be another portfolio construction procedure to allocate weights: <code>optimize</code> or <code>discrete allocation</code>. Defauts to <code>discrete_allocation</code>.</p> backtest_backup_params<pre><code>Optional[Dict[str, Any]] = dict(range_bound=\"mid\")\n</code></pre> <p>Backtest \"back-up\" configuration. Defauts to <code>dict(range_bound=\"mid\")</code> as <code>discrete_allocation</code> is set in <code>backtest_backup</code>.</p>"},{"location":"api/strategy/backtest/#example-backtest","title":"Example Backtest","text":"<p>Example Backtest<p>For this example, we are using <code>yfinance</code>, a python library to fetch yahoo market data. First, we import the module and query some tickers from 2019 to 2022: <pre><code>import yfinance as yf\n# Date range\nstart = \"2019-01-01\"\nend = \"2022-12-31\"\n# Tickers of assets\nassets = [\n\"JCI\", \"TGT\", \"CMCSA\", \"CPB\", \n\"MO\", \"APA\", \"MMC\", \"JPM\",\n\"ZION\", \"PSA\", \"BAX\", \"BMY\", \n\"LUV\", \"PCAR\", \"TXT\", \"TMO\",\n\"DE\", \"MSFT\", \"HPQ\", \"SEE\",\n]\n# Downloading data\ndata = yf.download(assets, start=start, end=end)\nstock_prices = data.loc[:, (\"Close\", slice(None))]\nstock_prices.columns = assets\n</code></pre></p> <pre><code>from opendesk.backtest import BacktestConfig\nfrom opendesk.blocks import Reversion, SignalBased, TrendFollowing\nfrom opendesk.strategy import Strategy\n# Config\nconfig = BacktestConfig(\nuniverse=close_prices,\nsteps=[\n(\"Reversion\", Reversion),\n(\"TrendFollowing\", TrendFollowing),\n],\nportfolio_construction=\"optimize\",\nportfolio_expected_returns_params=dict(\nmethod=\"capm_return\"\n),\nportfolio_cov_matrix_params=dict(\nmethod=\"sample_cov\"\n),\nsolver_method=\"efficient_risk\",\nsolver_params=dict(\ntarget_volatility=0.2, \nmarket_neutral=True\n),\nadd_constraints=[\nlambda w: w &lt;= 0.1, \nlambda w: w &gt;= -0.1\n],\nbacktest_every_nth=30,\n)\n</code></pre> <p> <pre><code>$ backtest = Strategy.backtest(config)\n$ backtest.total_profit(group_by=False)\n&lt;span style=\"color: grey;\"&gt;APA      20.123646\nBAX       6.338269\nBMY      -4.664290\nCMCSA     1.361760\nCPB       1.361636\nDE        8.268657\nHPQ       2.813247\nJCI      11.984073\nJPM       1.150170\nLUV      12.407130\nMMC      -4.770791\nMO       -4.857592\nMSFT      6.389380\nPCAR      5.609880\nPSA       4.577316\nSEE      -2.475789\nTGT      -7.691861\nTMO      -5.878451\nTXT       7.768601\nZION     -7.347374\nName: total_profit, dtype: float64\n&lt;/span&gt;\n</code></pre> </p> <p>More information about the vectorbt ecosystem can be found in the official documentation.</p> </p>"},{"location":"api/strategy/exposures/","title":"Exposures","text":""},{"location":"api/strategy/exposures/#add_blocks","title":"add_blocks","text":"<pre><code>Strategy.add_blocks(\n*args: Tuple[str, Type, Dict, Dict]\n) -&gt; opendesk.strategy.Strategy:\n</code></pre> <p>After initialization, additional strategies (blocks) can be added to the instance <code>steps</code>. We included in the below snippet two additionnal examples: A reversion factor and a Markov regime switching model.</p>"},{"location":"api/strategy/exposures/#parameters","title":"Parameters","text":"*args<pre><code>Tuple[str, Type, Dict, Dict]\n</code></pre> <p>Additional blocks to be added.</p>"},{"location":"api/strategy/exposures/#returns","title":"Returns","text":"<p><code>opendesk.strategy.Strategy</code> instance.</p> <p>Example Add Blocks</p> <pre><code>from opendesk import Strategy\nfrom opendesk.blocks import Reversion, MarkovRegression\nstrategy = Strategy(steps)\nother_blocks = (\n(\n\"reversion\",\nReversion, # (1)\n{'short_term_rule': None, 'long_term_rule': None}\n),\n(\n\"markov_regression\",\nMarkovRegression, # (2)\n{\"n_phase\": 3},\n{\"cli\": cli}\n)\n)\n</code></pre> <ol> <li>Calculate sentiment using Reversion Ranking Method.     More information provided in the Model Glossary.</li> <li>Calculate sentiment using Trend Following Ranking Method.     More information provided in the Model Glossary.</li> </ol> <p> <pre><code>$ strategy = Strategy(steps)\n &lt;span style=\"color: grey;\"&gt;Blocks -&gt; signal + gaussian_mixture&lt;/span&gt;\n$ strategy.add_blocks(other_blocks)\n&lt;span style=\"color: grey;\"&gt;Blocks -&gt; signal + gaussian_mixture + reversion + markov_regression&lt;/span&gt;\n</code></pre> </p>"},{"location":"api/strategy/exposures/#check_goup_constraints","title":"check_goup_constraints","text":"<pre><code>Strategy.check_group_constraints(\nweights: pandas.core.series.Series\n) \u2011&gt; pandas.core.frame.DataFrame\n</code></pre> <p>The performance of the model can be assessed by examining the satisfaction of predetermined constraints. By comparing the output of the model to the set of exposures produced by the actual allocation, it is possible to determine if it is operating as intended. To do this, we use the <code>check_group_constraints()</code> method, which groups assets and sum their weights.</p>"},{"location":"api/strategy/exposures/#parameters_1","title":"Parameters","text":"weights<pre><code>pandas.core.series.Series\n</code></pre> <p>Portfolio weights calculated either with the <code>optimize</code> or the <code>discrete_allocation</code> methods.</p>"},{"location":"api/strategy/exposures/#returns_1","title":"Returns","text":"<p><code>pandas.core.frame.DataFrame</code> object with calculated aggregated weights and target weights.</p> <p>Example Group Constraints</p> <pre><code>from opendesk import Strategy\nfrom opendesk.blocks import Reversion, TrendFollowing\nstrategy = Strategy(steps, topdown, mapping_table)\nstrategy.fit(df).estimate(sum).portfolio(stock_prices)    \nweights = strategy.discrete_allocation(\"equal_weighted\")\nseries_weights = pd.Series(weights, name=\"weights\")\n</code></pre> <p> <pre><code>$ strategy.check_group_constraints(series_weights))\n&lt;span style=\"color: grey;\"&gt;            weights  target weights\nsector 1      0.15    (0.05, 0.15)\nsector 10     0.10    (0.05, 0.15)\nsector 2      0.00      (0.0, 0.0)\nsector 3      0.00      (0.0, 0.0)\nsector 4     -0.05  (-0.15, -0.05)\nsector 5     -0.15  (-0.15, -0.05)\nsector 6      0.00      (0.0, 0.0)\nsector 7      0.00      (0.0, 0.0)\nsector 8     -0.05  (-0.15, -0.05)\nsector 9      0.00      (0.0, 0.0)\n&lt;/span&gt;\n</code></pre> </p>"},{"location":"api/strategy/exposures/#estimate","title":"estimate","text":"<pre><code>Strategy.estimate(\nfunc: Type, \ninplace: Optional[bool] = False\n) \u2011&gt; Union[pandas.core.series.Series, opendesk.strategy.Strategy]\n</code></pre> <p>Aggregate exposures by aggregating each units using a predetermined function <code>func</code>.</p>"},{"location":"api/strategy/exposures/#parameters_2","title":"Parameters","text":"func<pre><code>Type\n</code></pre> <p>Strategy exposures/tilts aggregated from model scores. The <code>func</code> parameter can be any object that is compatible with the <code>.apply</code> function in the pandas library.</p> inplace<pre><code>Optional[bool] = False\n</code></pre> <p>Returns a copy of <code>exposures</code>. Defaults to <code>False</code>.</p>"},{"location":"api/strategy/exposures/#returns_2","title":"Returns","text":"<p><code>opendesk.strategy.Strategy</code> instance.</p> <p>Example Estimate</p> <p>Some examples of different <code>func</code> parameter are provided below:</p> MeanMedianMaxMin <p> <pre><code>$ strategy.estimate(mean, inplace=True)\n&lt;span style=\"color: grey;\"&gt;sector 1     0.50\nsector 2     0.25\nsector 3     0.50\nsector 4    -0.75\nsector 5     0.00\nsector 6    -0.50\nsector 7    -1.00\nsector 8     0.50\nsector 9    -0.25\nsector 10    0.75\nName: mean, dtype: float64\n&lt;/span&gt;\n</code></pre> </p> <p> <pre><code>$ from statistics import median\n$ strategy.estimate(median, inplace=True)\n&lt;span style=\"color: grey;\"&gt;sector 1     0.5\nsector 2     0.5\nsector 3     1.0\nsector 4    -0.5\nsector 5    -0.5\nsector 6    -0.5\nsector 7    -1.5\nsector 8     0.5\nsector 9     0.0\nsector 10    0.5\nName: median, dtype: float64\n&lt;/span&gt;\n</code></pre> </p> <p> <pre><code>$ from statistics import median\n$ strategy.estimate(median, inplace=True)\n&lt;span style=\"color: grey;\"&gt;sector 1     2\nsector 2     2\nsector 3     2\nsector 4     0\nsector 5     2\nsector 6     1\nsector 7     1\nsector 8     2\nsector 9     0\nsector 10    2\nName: max, dtype: int64\n&lt;/span&gt;\n</code></pre> </p> <p> <pre><code>$ from statistics import median\n$ strategy.estimate(median, inplace=True)\n&lt;span style=\"color: grey;\"&gt;sector 1    -1\nsector 2    -2\nsector 3    -2\nsector 4    -2\nsector 5    -1\nsector 6    -2\nsector 7    -2\nsector 8    -1\nsector 9    -1\nsector 10    0\nName: min, dtype: int64\n&lt;/span&gt;\n</code></pre> </p>"},{"location":"api/strategy/exposures/#fit","title":"fit","text":"<pre><code>Strategy.fit(\nmodel_data: pandas.core.frame.DataFrame, \nbackend: Optional[str] = 'joblib', \nverbose: Optional[bool] = True\n) \u2011&gt; opendesk.strategy.Strategy\n</code></pre> <p>Executes each provided blocks with common dataset.</p>"},{"location":"api/strategy/exposures/#parameters_3","title":"Parameters","text":"model_data<pre><code>pandas.core.frame.DataFrame\n</code></pre> <p>Market returns time-series used to train the model.</p> backend<pre><code>Optional[str] = \"joblib\"\n</code></pre> <p>Run parallel multiprocessing or iterative process. Defaults to <code>\"joblib\"</code>.</p> verbose<pre><code>Optional[bool] = True\n</code></pre> <p>Joblib progress messages are printed. Defaults to <code>True</code>.</p>"},{"location":"api/strategy/exposures/#returns_3","title":"Returns","text":"<p><code>opendesk.strategy.Strategy</code> instance.</p> <p>Example Fit</p> <p>After initializing the class, the market returns can be fit using the <code>fit()</code> method, as in the following example:</p> <pre><code>strategy = Strategy(steps).fit(df)\n</code></pre> <p>This returns the instance object <code>self</code> on which the method was called. </p>"},{"location":"api/strategy/portfolio_construction/","title":"Porfolio","text":"<p>Portfolio construction is the process of creating a balanced collection of investments that aligns with an investor's financial goals, risk tolerance, and investment horizon. The goal of portfolio construction is to maximize returns while minimizing risk.</p> <p>Portfolio construction involves translating scores into weights, can be a complex and nuanced process. We have developed two methods that allows for greater flexibility and experimentation: </p> <ul> <li>Optimization</li> <li>Discrete Allocation</li> </ul> <p>These approach enables the exploration of a wide range of potential top-down and bottom-up portfolio compositions.</p>"},{"location":"api/strategy/portfolio_construction/#portfolio","title":"portfolio","text":"<pre><code>Strategy.portfolio(\ndata: Optional[pandas.core.frame.DataFrame] = None, \n) \u2011&gt; opendesk.strategy.Strategy\n</code></pre>"},{"location":"api/strategy/portfolio_construction/#parameters","title":"Parameters","text":"data<pre><code>Optional[pandas.core.frame.DataFrame] = None\n</code></pre> <p>Market price time-series, each row is a date and each column is a ticker/id. If <code>None</code>, it takes <code>model_data</code>, the dataset used in the <code>fit()</code> method. Defaults to <code>None</code>.</p>"},{"location":"api/strategy/portfolio_construction/#returns","title":"Returns","text":"<p><code>opendesk.strategy.Strategy</code> instance.</p>"},{"location":"api/strategy/portfolio_construction/#portfolioconstruction","title":"PortfolioConstruction","text":"<pre><code>portfolio.PortfolioConstruction(\ndata: pandas.core.frame.DataFrame,\ngroup_constraints: Optional[Dict[str, Tuple[float, float]]] = None,\nexposures: Optional[pandas.core.series.Series] = None,\ntopdown: Optional[bool] = False,\nmapping_table: Optional[Dict[str, str]] = None,\nfreq: Optional[int] = 252\n)\n</code></pre> <p>Portfolio construction is the process of creating a balanced collection of investments that aligns with an investor's financial goals, risk tolerance, and investment horizon. The goal of portfolio construction is to maximize returns while minimizing risk.</p>"},{"location":"api/strategy/portfolio_construction/#parameters_1","title":"Parameters","text":"data<pre><code>pandas.core.frame.DataFrame\n</code></pre> <p>Adjusted closing prices of the asset, each row is a date and each column is a ticker/id.</p> group_constraints<pre><code>Optional[Dict[str, Tuple[float, float]]] = None\n</code></pre> <p>Strategy constraints by group. Product of <code>mapping_weights</code> inputs and <code>exposures</code> outputs from the <code>Strategy</code> class.</p> exposures<pre><code>Optional[pandas.core.series.Series] = None\n</code></pre> <p>Strategy exposures attribute estimated from the <code>estimate()</code> function. </p> <p>In the portfolio optimization section, it also core to the <code>asset_views</code> property, to limit the overall average score (exposure) as a custom constraint. E.i. suppose that for each asset you have some \u201cscore\u201d \u2013 it could be an ESG metric, or some custom risk/return metric. It is simple to specify linear constraints, like \u201cportfolio ESG score must be greater than x\u201d: you simply create a vector of scores, add a constraint on the dot product of those scores with the portfolio weights, then optimize your objective:</p> <p>Example</p> <pre><code>from opendesk.blocks import ESGModel\n# portolfio mininum score to find\nportfolio_min_score = 0.5\n# start strategy\nesg_strategy = Strategy(steps=[(\"ESG\", ESGModel)], topdown=True, mapping_table=mapping_table)\nesg_strategy.fit(df).estimate(sum) # (1)\nesg_strategy.optimize(stock_prices)\nesg_strategy.portfolio(**portfolio_params) # (2)\nesg_strategy.add(\ncustom_constraints=[\nlambda w: strategy.asset_scores @ w &gt;= portfolio_min_score\n]\n)\n</code></pre> <ol> <li>Here, ESG scores are produced by the strategy <code>estimate()</code> function.</li> <li>Portfolio parameters are not explained here, as the goal of this snippet is to showcase the <code>custom_constraints</code> parameter with the optimizer <code>asset_scores</code> proprety.</li> </ol> topdown<pre><code>Optional[bool] = False\n</code></pre> <p>Activates top-down strategy. The strategy tilts is processed at a higher level (e.i. sector level) than the actual allocation exectution (e.i. stock level). If set to True, a mapping table should be passed. Defaults to False.</p> mapping_table<pre><code>Optional[Dict[str, str]] = None\n</code></pre> <p>Maps higher with lower level assets. Variable <code>topdown</code> needs to be turned to <code>True</code>. Defaults to <code>None</code>.</p> freq<pre><code>Optional[int] = 252\n</code></pre> <p>Number of time periods in a year, Defaults to 252 (the number of trading days in a year).</p>"},{"location":"api/strategy/portfolio_construction/#descendants","title":"Descendants","text":"<ul> <li>opendesk.strategy.Strategy</li> </ul>"},{"location":"api/strategy/portfolio_construction/#instance-variables","title":"Instance variables","text":""},{"location":"api/strategy/portfolio_construction/#asset_scores","title":"asset_scores","text":"asset_scores<pre><code>Dict[str, float]\n</code></pre> <p>Transform exposures to score at any level. If <code>topdown</code> is set to <code>True</code>, it transforms exposures at the lower level. Otherwise, It returns <code>exposures</code>.</p>"},{"location":"api/strategy/portfolio_construction/#asset_views","title":"asset_views","text":"asset_views<pre><code>Dict[str, float]\n</code></pre> <p>The alpha blocks implementation works with the Black-Litterman <code>asset_views</code>, where views direction is extracted from the median of weight range and the magnitude is \\(1.96 \\sigma\\), where \\(\\sigma\\) is the annualized volatility calculated from log returns.</p> <p>1.96 is Hardcoded</p> <p>In probability and statistics, the 97.5th percentile point of the standard normal distribution is a number commonly used for statistical calculations. The approximate value of this number is 1.96, meaning that 95% of the area under a normal curve lies within approximately 1.96 standard deviations of the mean.</p>"},{"location":"api/strategy/portfolio_construction/#lower_bound","title":"lower_bound","text":"lower_bound<pre><code>Dict[str, Tuple]\n</code></pre> <p>Lower weight level constraints by group, from <code>group_constraints</code>.</p>"},{"location":"api/strategy/portfolio_construction/#mid_bound","title":"mid_bound","text":"mid_bound<pre><code>Dict[str, Tuple]\n</code></pre> <p>Mid weight level constraints by group, from <code>group_constraints</code>.</p>"},{"location":"api/strategy/portfolio_construction/#upper_bound","title":"upper_bound","text":"upper_bound<pre><code>Dict[str, Tuple]\n</code></pre> <p>Upper weight level constraints by group, from <code>group_constraints</code>.</p>"},{"location":"api/strategy/portfolio_construction/#public-methods","title":"Public Methods","text":"<ul> <li><code>discrete_allocation()</code>: Set portfolio weights following a discrete allocation weighting scheme</li> <li><code>optimize()</code>: Portfolio optimization, which aims to select the optimal mix of assets in a portfolio in order to satisfy the defined objectives and constraints</li> </ul>"},{"location":"api/strategy/portfolio_construction/discrete_allocation/","title":"Discrete Allocation","text":"<p>The <code>portfolio()</code> calls the <code>PortfolioConstruction</code> class, which includes the implementation of the <code>discrete_allocation()</code> built-in public methods for discrete allocation procedures, which allows multiple pre-determined rule-based allocation strategies</p>"},{"location":"api/strategy/portfolio_construction/discrete_allocation/#discrete_allocation","title":"discrete_allocation","text":"<pre><code>Portfolio.discrete_allocation(\nmodel: str,\nmodel_params: Dict[str, Any] = None,\nrange_bound: Optional[str] = 'mid'\n) \u2011&gt; OrderedDict\n</code></pre> <p>Discrete allocation is a method for implementing predefined, rule-based allocation strategies for building optimal, diversified portfolios at scale. The use of the <code>topdown</code> parameter, when set to <code>True</code>, can introduce additional complexity in the portfolio construction process, as it involves diversifying allocation across a wide range of assets, using techniques such as uniform or market cap-based allocation. The <code>PortfolioConstruction.discrete_allocation()</code> function calls the <code>DiscreteAllocation</code> class (only when <code>topdown=True</code>), which offers a variety of rule-based weighting schemes. These weighting schemes, which are commonly used to construct factor portfolios, are designed to achieve a range of portfolio objectives.</p>"},{"location":"api/strategy/portfolio_construction/discrete_allocation/#parameters","title":"Parameters","text":"model<pre><code>Optional[str] = \"equal_weighted\"\n</code></pre> <p>Model used to allocate weights. Possible methods are:</p> <ul> <li><code>equal_weighted</code>: Asset equally weighted</li> <li><code>market_cap_weighted</code>: Asset weighted in proportion to their free-float market cap</li> <li><code>score_weighted</code>: Asset weighted in proportion to their target-factor scores</li> <li><code>score_tilt_weighted</code>: Asset weighted in proportion to the product of their market cap and factor score</li> <li><code>inv_volatility_weighted</code>: Asset weighted in proportion to the inverse of their historical volatility</li> <li><code>min_correlation_weighted</code>: Optimized weighting scheme to obtain a portfolio with minimum volatility under the assumption that all asset have identical volatilities</li> </ul> <p>Defaults to <code>equal_weighted</code>.</p> model_params<pre><code>Dict[str, Any] = None\n</code></pre> <p>Model specific parameters.</p> range_bound<pre><code>Optional[str] = \"mid\"\n</code></pre> <p>Weight bound (from <code>mapping_weights</code>). Total budget (in %) to apply. Possible values are:</p> <ul> <li><code>lower</code>: Lower weight bound</li> <li><code>mid</code>: Median weight</li> <li><code>upper</code>: Upper weight bound</li> </ul> <p>Defaults to <code>mid</code>.</p>"},{"location":"api/strategy/portfolio_construction/discrete_allocation/#returns","title":"Returns","text":"<p><code>OrderedDict</code>, discrete weights.</p>"},{"location":"api/strategy/portfolio_construction/discrete_allocation/#example-discrete-allocation","title":"Example Discrete Allocation","text":"<p>Example Discrete Allocation</p> <pre><code>from opendesk import Strategy\nstrategy = Strategy(steps=steps, topdown=True, mapping_table=mapping_table)\nstrategy.fit(df).estimate(sum)\nweights = strategy.portfolio(stock_prices).discrete_allocation(model=\"equal_weighted\")\n</code></pre>"},{"location":"api/strategy/portfolio_construction/optimization/","title":"Optimization","text":"<p>Portfolio optimization capabilities is the process of selecting the optimal mix of assets in a portfolio, with respect to the alpha scores, in order to maximize returns while minimizing risk. The <code>portfolio()</code> method has been implemented to streamline the process of optimization and facilitate the integration of backtesting.</p> <p>The <code>portfolio()</code> calls the <code>PortfolioConstruction</code> class, which includes the implementation of the <code>optimize()</code> built-in public methods for optimization procedures, which integrates vectorbt pro PyPortfolioOpt wrapper.</p>"},{"location":"api/strategy/portfolio_construction/optimization/#integration","title":"Integration","text":"<p>PyPortfolioOpt is a library that implements portfolio optimization methods, including classical efficient frontier techniques and Black-Litterman allocation, as well as more recent developments in the field like shrinkage and Hierarchical Risk Parity, along with some novel experimental features, like exponentially-weighted covariance matrices. </p> <p>PyPortfolioOpt implements a range of optimization methods that are very easy to use. The optimization procedure consists of several distinct steps (some of them may be skipped depending on the optimizer):</p> <ul> <li>Calculate the expected returns (mostly located in <code>pypfopt.expected_returns</code>)</li> <li>Calculate the covariance matrix (mostly located in <code>pypfopt.risk_models</code>)</li> <li>Initialize and set up an optimizer (mostly located in <code>pypfopt.efficient_frontier</code>, with the base class located in `pypfopt.base_optimizer) including objectives, constraints, and target</li> <li>Run the optimizer to get the weights</li> <li>Convert the weights into a discrete allocation (optional)</li> </ul> <p>For example, let's perform the mean-variance optimization (MVO) for maximum Sharpe:</p> <pre><code>from pypfopt.expected_returns import mean_historical_return\nfrom pypfopt.risk_models import CovarianceShrinkage\nfrom pypfopt.efficient_frontier import EfficientFrontier\nexpected_returns = mean_historical_return(data)\ncov_matrix = CovarianceShrinkage(data).ledoit_wolf()\noptimizer = EfficientFrontier(expected_returns, cov_matrix)\nweights = optimizer.max_sharpe()\n</code></pre>"},{"location":"api/strategy/portfolio_construction/optimization/#parsing","title":"Parsing","text":"<p>The entire codebase of PyPortfolioOpt (with a few exceptions) has consistent argument and function namings, such that we can build a semantic web of functions acting as inputs to other functions. Therefore, the user just needs to provide the target function (e.g. <code>EfficientFrontier.max_sharpe</code>), and we can programmatically figure out the entire call stack having the pricing data alonw. If the user passes any additional keyword arguments, we can check which functions from the stack accept those arguments and automatically pass them.</p> <p>For the example above, the web would be:</p> <pre><code>flowchart TD\n    U[User] -- prices --&gt; m[mean_historical_return]\n    U -- prices --&gt; c[CovarianceShrinkage.ledoit_wolf]\n    m -- expected_returns --&gt; e[EfficientFrontier]\n    c -- cov_matrix --&gt; e</code></pre>"},{"location":"api/strategy/portfolio_construction/optimization/#auto-optimization","title":"Auto-optimization","text":"<p>Knowing how to parse and resolve function arguments, vectorbt implements a function <code>pypfopt_optimize</code>, which takes user requirements and translates them into function calls:</p> <pre><code>vbt.pypfopt_optimize(prices=data.get(\"Close\"))\n</code></pre> <p>In short, <code>pypfopt_optimize</code> triggers a waterfall of argument resolutions by parsing arguments, including the calculation of the expected returns and the risk model quantifying asset risk. Then, it adds objectives and constraints to the optimizer instance. Finally, it calls the target metric method (such as <code>max_sharpe</code>) or custom convex/non-convex objective using the same parsing procedure as we did above. If wanted, it can also translate continuous weights into discrete ones using <code>pypfopt.DiscreteAllocation</code>.</p> <p>Since multiple PyPortfolioOpt functions can require the same argument that has to be pre-computed yet, pypfopt_optimize deploys a built-in caching mechanism. Below, we will demonstrate various optimizations done both using PyPortfolioOpt and vectorbt:</p> <p>Optimizing a long/short portfolio to minimise total variance:</p> pypfoptvectorbtopendesk <pre><code>S = CovarianceShrinkage(data.get(\"Close\")).ledoit_wolf()\nef = EfficientFrontier(None, S, weight_bounds=(-1, 1))\nef.min_volatility()\nweights = ef.clean_weights()\n</code></pre> <pre><code>vbt.pypfopt_optimize(  \nprices=data.get(\"Close\"),\nexpected_returns=None,\nweight_bounds=(-1, 1),\ntarget=\"min_volatility\"\n)\n</code></pre> <pre><code>strategy.portfolio(data.get(\"Close\")).optimize(  \nexpected_returns=None,\nweight_bounds=(-1, 1),\ntarget=\"min_volatility\"\n)\n</code></pre> <p>Optimizing a portfolio to maximise the Sharpe ratio, subject to sector constraints. In opendesk, while the sector constraints is also an option, we can set the <code>alpha_block_constraints</code> to True, which constraints the portfolio depending on pre-modeled alpha blocks exposures:</p> pypfoptvectorbtopendesk <pre><code>from pypfopt.expected_returns import capm_return\nmu = capm_return(data.get(\"Close\"))\nS = CovarianceShrinkage(data.get(\"Close\")).ledoit_wolf()\nef = EfficientFrontier(mu, S)\nef.add_sector_constraints(sector_mapper, sector_lower, sector_upper)\nef.max_sharpe()\nweights = ef.clean_weights()\nweights\n</code></pre> <pre><code>vbt.pypfopt_optimize(  \nprices=data.get(\"Close\"),\nexpected_returns=\"capm_return\",\nsector_mapper=sector_mapper,\nsector_lower=sector_lower,\nsector_upper=sector_upper,\n)\n</code></pre> <pre><code>strategy.portfolio(data.get(\"Close\")).optimize(  \nexpected_returns=\"capm_return\",\nalpha_block_constraints=True\n)\n</code></pre> <p>Optimizing a portfolio to maximise return for a given risk, subject to sector constraints, with an L2 regularisation objective:</p> pypfoptvectorbtopendesk <pre><code>from pypfopt.objective_functions import L2_reg\nmu = capm_return(data.get(\"Close\"))\nS = CovarianceShrinkage(data.get(\"Close\")).ledoit_wolf()\nef = EfficientFrontier(mu, S)\nef.add_sector_constraints(sector_mapper, sector_lower, sector_upper)\nef.add_objective(L2_reg, gamma=0.1)\nef.efficient_risk(0.15)\nweights = ef.clean_weights()\n</code></pre> <pre><code>vbt.pypfopt_optimize(  \nprices=data.get(\"Close\"),\nexpected_returns=\"capm_return\",\nsector_mapper=sector_mapper,\nsector_lower=sector_lower,\nsector_upper=sector_upper,\nobjectives=[\"L2_reg\"],  \ngamma=0.1,  \ntarget=\"efficient_risk\",\ntarget_volatility=0.15  \n)\n</code></pre> <pre><code>strategy.portfolio(data.get(\"Close\")).optimize(  \nexpected_returns=\"capm_return\",\nsector_mapper=sector_mapper,\nsector_lower=sector_lower,\nsector_upper=sector_upper,\nobjectives=[\"L2_reg\"],  \ngamma=0.1,  \ntarget=\"efficient_risk\",\ntarget_volatility=0.15  \n)\n</code></pre> <p>Optimizing along the mean-semivariance frontier:</p> pypfoptvectorbtopendesk <pre><code>from pypfopt import EfficientSemivariance\nfrom pypfopt.expected_returns import returns_from_prices\nmu = capm_return(data.get(\"Close\"))\nreturns = returns_from_prices(data.get(\"Close\"))\nreturns = returns.dropna()\nes = EfficientSemivariance(mu, returns)\nes.efficient_return(0.01)\nweights = es.clean_weights()\n</code></pre> <pre><code>vbt.pypfopt_optimize(\nprices=data.get(\"Close\"),\nexpected_returns=\"capm_return\",\noptimizer=\"efficient_semivariance\",  \ntarget=\"efficient_return\",\ntarget_return=0.01\n)\n</code></pre> <pre><code>strategy.portfolio(data.get(\"Close\")).optimize(  \nexpected_returns=\"capm_return\",\noptimizer=\"efficient_semivariance\",  \ntarget=\"efficient_return\",\ntarget_return=0.01\n)\n</code></pre> <p>Minimizing transaction costs:</p> <pre><code>initial_weights = np.array([1 / len(data.symbols)] * len(data.symbols))\n</code></pre> pypfoptvectorbtopendesk <pre><code>from pypfopt.objective_functions import transaction_cost\nmu = mean_historical_return(data.get(\"Close\"))\nS = CovarianceShrinkage(data.get(\"Close\")).ledoit_wolf()\nef = EfficientFrontier(mu, S)\nef.add_objective(transaction_cost, w_prev=initial_weights, k=0.001)\nef.add_objective(L2_reg, gamma=0.05)\nef.min_volatility()\nweights = ef.clean_weights()\n</code></pre> <pre><code>vbt.pypfopt_optimize(  \nprices=data.get(\"Close\"),\nobjectives=[\"transaction_cost\", \"L2_reg\"],\nw_prev=initial_weights, \nk=0.001,\ngamma=0.05,\ntarget=\"min_volatility\"\n)\n</code></pre> <pre><code>strategy.portfolio(data.get(\"Close\")).optimize(  \nobjectives=[\"transaction_cost\", \"L2_reg\"],\nw_prev=initial_weights, \nk=0.001,\ngamma=0.05,\ntarget=\"min_volatility\"\n)\n</code></pre> <p>Custom convex objective:</p> <pre><code>import cvxpy as cp\ndef logarithmic_barrier_objective(w, cov_matrix, k=0.1):\nlog_sum = cp.sum(cp.log(w))\nvar = cp.quad_form(w, cov_matrix)\nreturn var - k * log_sum\n</code></pre> pypfoptvectorbtopendesk <pre><code>mu = mean_historical_return(data.get(\"Close\"))\nS = CovarianceShrinkage(data.get(\"Close\")).ledoit_wolf()\nef = EfficientFrontier(mu, S, weight_bounds=(0.01, 0.3))\nef.convex_objective(logarithmic_barrier_objective, cov_matrix=S, k=0.001)\nweights = ef.clean_weights()\n</code></pre> <pre><code>vbt.pypfopt_optimize(  \nprices=data.get(\"Close\"),\nweight_bounds=(0.01, 0.3),\nk=0.001,\ntarget=logarithmic_barrier_objective  \n)\n</code></pre> <pre><code>strategy.portfolio(data.get(\"Close\")).optimize(  \nweight_bounds=(0.01, 0.3),\nk=0.001,\ntarget=logarithmic_barrier_objective  \n)\n</code></pre> <p>Custom non-convex objective:</p> <pre><code>def deviation_risk_parity(w, cov_matrix):\ncov_matrix = np.asarray(cov_matrix)\nn = cov_matrix.shape[0]\nrp = (w * (cov_matrix @ w)) / cp.quad_form(w, cov_matrix)\nreturn cp.sum_squares(rp - 1 / n).value\n</code></pre> pypfoptvectorbtopendesk <pre><code>mu = mean_historical_return(data.get(\"Close\"))\nS = CovarianceShrinkage(data.get(\"Close\")).ledoit_wolf()\nef = EfficientFrontier(mu, S)\nef.nonconvex_objective(deviation_risk_parity, ef.cov_matrix)\nweights = ef.clean_weights()\n</code></pre> <pre><code>vbt.pypfopt_optimize(  \nprices=data.get(\"Close\"),\ntarget=deviation_risk_parity,  \ntarget_is_convex=False\n)\n</code></pre> <pre><code>strategy.portfolio(data.get(\"Close\")).optimize(  \ntarget=deviation_risk_parity,  \ntarget_is_convex=False\n)\n</code></pre> <p>Black-Litterman Allocation, where <code>viewdict</code> is integrated within alpha blocks methods, where views are extracted from the median of weight range and magnitude calculated by multiplying the annualized volatility times \\(1.96\\) (95% of the area under a normal curve lies within approximately 1.96 standard deviations of the mean):</p> <pre><code>sp500_data = vbt.YFData.fetch(\n\"^GSPC\", \nstart=data.wrapper.index[0], \nend=data.wrapper.index[-1]\n)\n# example given in vectorbt\nmarket_caps = data.get(\"Close\") * data.get(\"Volume\")\nviewdict = {\n\"ADAUSDT\": 0.20, \n\"BNBUSDT\": -0.30, \n\"BTCUSDT\": 0, \n\"ETHUSDT\": -0.2, \n\"XRPUSDT\": 0.15\n}\n</code></pre> pypfoptvectorbtopendesk <pre><code>from pypfopt.black_litterman import (\nmarket_implied_risk_aversion,\nmarket_implied_prior_returns,\nBlackLittermanModel\n)\nS = CovarianceShrinkage(data.get(\"Close\")).ledoit_wolf()\ndelta = market_implied_risk_aversion(sp500_data.get(\"Close\"))\nprior = market_implied_prior_returns(market_caps.iloc[-1], delta, S)\nbl = BlackLittermanModel(S, pi=prior, absolute_views=viewdict)\nrets = bl.bl_returns()\nef = EfficientFrontier(rets, S)\nef.min_volatility()\nweights = ef.clean_weights()\n</code></pre> <pre><code>vbt.pypfopt_optimize(  \nprices=data.get(\"Close\"),\nexpected_returns=\"bl_returns\",  \nmarket_prices=sp500_data.get(\"Close\"),\nmarket_caps=market_caps.iloc[-1],\nabsolute_views=viewdict,\ntarget=\"min_volatility\"\n)\n</code></pre> <pre><code>strategy.portfolio(data.get(\"Close\")).optimize(  \nexpected_returns=\"bl_returns\",  \nmarket_prices=sp500_data.get(\"Close\"),\nmarket_caps=market_caps.iloc[-1],\nabsolute_views='alpha_blocks',\ntarget=\"min_volatility\"\n)\n</code></pre> <p>Hierarchical Risk Parity:</p> pypfoptvectorbtopendesk <pre><code>from pypfopt import HRPOpt\nrets = returns_from_prices(data.get(\"Close\"))\nhrp = HRPOpt(rets)\nhrp.optimize()\nweights = hrp.clean_weights()\n</code></pre> <pre><code>vbt.pypfopt_optimize(\nprices=data.get(\"Close\"),  \noptimizer=\"hrp\",\ntarget=\"optimize\"\n)\n</code></pre> <pre><code>strategy.portfolio(data.get(\"Close\")).optimize(  \noptimizer=\"hrp\",\ntarget=\"optimize\"\n)\n</code></pre>"},{"location":"api/strategy/portfolio_construction/optimization/#argument-groups","title":"Argument groups","text":"<p>In cases where two functions require an argument with the same name but you want to pass different values to them, pass the argument as an instance of <code>pfopt_func_dict</code> where keys should be functions or their names, and values should be different argument values:</p> <pre><code>vbt.pypfopt_optimize(  \nprices=data.get(\"Close\"),\nexpected_returns=\"bl_returns\",  \nmarket_prices=sp500_data.get(\"Close\"),\nmarket_caps=market_caps.iloc[-1],\nabsolute_views=viewdict,\ntarget=\"min_volatility\",\ncov_matrix=vbt.pfopt_func_dict({\n\"EfficientFrontier\": \"sample_cov\",  \n\"_def\": \"ledoit_wolf\"  \n})\n)\n</code></pre>"},{"location":"api/strategy/portfolio_construction/optimization/#optimize","title":"optimize","text":"<pre><code>Portfolio.optimize(\ntarget: Optional[Union[Callable, str]] = None,\ntarget_is_convex: Optional[bool] = None,\nweights_sum_to_one: Optional[bool] = None,\ntarget_constraints: Optional[List[Any]] = None,\ntarget_solver: Optional[str] = None,\ntarget_initial_guess: Optional[np.array] = None,\nobjectives: Optional[List[Union[Callable, str]]] = None,\nconstraints: Optional[List[Callable]] = None,\nalpha_block_constraints: Optional[bool] = True,\nsector_mapper: Optional[dict] = None,\nsector_lower: Optional[dict] = None,\nsector_upper: Optional[dict] = None,\ndiscrete_allocation: Optional[bool] = None,\nallocation_method: Optional[str] = None,\nsilence_warnings: Optional[bool] = None,\nignore_opt_errors: Optional[bool] = None,\n**kwargs\n) -&gt; OrderedDict:\n</code></pre> <p>Base optimizer model, allowing for the efficient computation of optimized asset weights. The portfolio method houses different optimization methods from PyPortfolioOpt, which generate optimal portfolios for various possible objective functions and parameters.</p> <p>New Object Instantiation</p> <p>A new object should be instantiated if you want to make any change to objectives/constraints/bounds/parameters.</p>"},{"location":"api/strategy/portfolio_construction/optimization/#parameters","title":"Parameters","text":"target<pre><code>Optional[Union[Callable, str]] = \"max_sharpe\"\n</code></pre> <ul> <li><code>min_volatility</code>: Optimizes for minimum volatility</li> <li><code>min_semivariance</code>: Minimises the portfolio semi-variance (downside deviation)</li> <li><code>max_sharpe()</code>: Optimizes for maximal Sharpe ratio (a.k.a the tangency portfolio)</li> <li><code>max_quadratic_utility</code>: Maximises the quadratic utility, given some risk aversion</li> <li><code>efficient_risk</code>: Maximises return for a given target risk</li> <li><code>efficient_return</code>: Minimises risk for a given target return</li> <li><code>min_cvar</code>: Minimises the CVaR</li> <li><code>min_cdar</code>: Minimises the CDaR</li> </ul> <p>Custom Objectives: The Kelly Criterion</p> <pre><code>def kelly_objective(w, e_returns, cov_matrix, k=3): # (1)\nvariance = np.dot(w.T, np.dot(cov_matrix, w))\nobjective = variance * 0.5 * k - np.dot(w, e_returns)\nreturn objective\nweights = strategy.portfolio(prices).optimize(target=kelly_objective)\n</code></pre> <ol> <li>In probability theory, the Kelly criterion is a formula that determines the optimal theoretical size for a bet. It is valid when the expected returns are known. The Kelly bet size is found by maximizing the expected value of the logarithm of wealth, which is equivalent to maximizing the expected geometric growth rate, J. L. Kelly Jr [1956]. The criterion is also known as the scientific gambling method, as it leads to higher wealth compared to any other strategy in the long run (i.e. the theoretical maximum return as the number of bets goes to infinity).</li> </ol> target_is_convex<pre><code>Optional[bool] = None\n</code></pre> <p>If <code>target_is_convex</code> is True, the function is added as a convex function. Otherwise, the function is added as a non-convex function.</p> weights_sum_to_one<pre><code>Optional[bool] = None\n</code></pre> <p>Convex objective. Forces the sum of weight equals 1.</p> target_constraints<pre><code>Optional[List[Any]] = None\n</code></pre> target_solver<pre><code> Optional[str] = None\n</code></pre> target_initial_guess<pre><code>Optional[np.array] = None\n</code></pre> objectives<pre><code>Optional[List[Union[Callable, str]]] = None\n</code></pre> <p>List of lambda functions to add new term into the based objective function. This term must be convex, and built from cvxpy atomic functions.</p> <p>Built-in objective functions wrapper includes:</p> <ul> <li> <p><code>L2_reg</code>: L2 regularisation, i.e \\(\\gamma ||w||^2\\), to increase the number of nonzero weights. Mean-variance optimization often results in many weights being negligible, i.e the efficient portfolio does not end up including most of the assets. This is expected behaviour, but it may be undesirable if you need a certain number of assets in your portfolio. In order to coerce the mean-variance optimizer to produce more non-negligible weights, we add what can be thought of as a \u201csmall weights penalty\u201d to all of the objective functions, parameterised by \\(\\gamma\\) (gamma). This term reduces the number of negligible weights, because it has a minimum value when all weights are equally distributed, and maximum value in the limiting case where the entire portfolio is allocated to one asset. We refer to it as L2 regularisation because it has exactly the same form as the L2 regularisation term in machine learning, though a slightly different purpose (in ML it is used to keep weights small while here it is used to make them larger).</p> </li> <li> <p><code>ex_ante_tracking_error</code>: Calculate the (square of) the ex-ante Tracking Error, i.e \\((w - w_b)^T \\Sigma (w-w_b)\\)</p> </li> <li><code>ex_post_tracking_error</code>: Calculate the (square of) the ex-post Tracking Error, i.e \\(Var(r - r_b)\\)</li> <li><code>portfolio_return</code>: Calculate the (negative) mean return of a portfolio</li> <li><code>portfolio_variance</code>: Calculate the total portfolio variance (i.e square volatility)</li> <li><code>quadratic_utility</code>: Quadratic utility function, i.e \\(\\mu - \\frac 1 2 \\delta  w^T \\Sigma w\\)</li> <li><code>sharpe_ratio</code>: Calculate the (negative) Sharpe ratio of a portfolio</li> <li><code>transaction_cost</code>: A very simple transaction cost model: sum all the weight changes and multiply by a given fraction (default to 10bps). This simulates a fixed percentage commission from your broker.</li> </ul> gamma<pre><code>Optional[float] = 1\n</code></pre> <p>L2 regularisation parameter, defaults to 1. Increase if you want more non-negligible weights. In practice, \\(\\gamma\\) must be tuned to achieve the level of regularisation that you want. However, if the universe of assets is small (less than 20 assets), then gamma=1 is a good starting point. For larger universes, or if you want more non-negligible weights in the final portfolio, increase gamma.</p> k<pre><code>Optional[float] = 0.001\n</code></pre> <p>When transaction cost objective is set, fractional cost per unit weight exchanged.</p> constraints<pre><code>Optional[List[Callable]] = None\n</code></pre> <p>List of lambda function (e.i. all assets &lt;= 3% of the total portfolio = [lambda w: w &lt;= .03]. This constraint must satisfy DCP rules, i.e be either a linear equality constraint or convex inequality constraint.</p> alpha_block_constraints<pre><code>Optional[bool] = True\n</code></pre> <p>Alpha blocks core constraints. It adds constraints on the sum of weights of different groups of assets. Most commonly, these will be sector constraints. These constraints a particularly relevant when working with alpha blocks (top-down or bottom-up), as we aim to limit our exposure to paricular group of assets. Defaults to <code>True</code>.</p> sector_mapper<pre><code>Optional[dict] = None\n</code></pre> <p>Maps tickers to sectors. Equivalent to <code>Strategy.mapping_table</code>.</p> sector_lower<pre><code>Optional[dict] = None\n</code></pre> <p>Lower bounds for each sector. Equivalent to <code>Portfolio.lower_bound</code>.</p> sector_upper<pre><code>Optional[dict] = None\n</code></pre> <p>Upper bounds for each sector. Equivalent to <code>Portfolio.upper_bound</code>.</p> discrete_allocation<pre><code>Optional[bool] = None\n</code></pre> <p>The discrete_allocation module contains the <code>DiscreteAllocation</code> class, which offers multiple methods to generate a discrete portfolio allocation from continuous weights. If <code>discrete_allocation</code> is True, it calls <code>allocation_method</code> as an attribute of the allocation object.</p> allocation_method<pre><code>Optional[str] = \"lp_portfolio\"\n</code></pre> <p>Generate a discrete portfolio allocation from continuous weights.  Method are:</p> <ul> <li><code>greedy_portfolio</code>: Uses a greedy algorithm</li> <li><code>lp_portfolio</code>: Uses linear programming</li> </ul> silence_warnings<pre><code>Optional[bool] = None\n</code></pre> <p>Disable warnings. If set to False, it throws a warning stating that either the optimization fails or that an argument wasn't required by any function in the call stack. </p> ignore_opt_errors<pre><code>Optional[bool] = None\n</code></pre> <p>Ignore any target optimization errors.</p> optimizer<pre><code>Optional[str] = None\n</code></pre> <p>Specify the optimizer. It can be an instance of <code>pypfopt.base_optimizer.BaseOptimizer</code>, an attribute of <code>pypfopt</code>, a subclass of <code>pypfopt.base_optimizer.BaseOptimizer</code>, or one of the following options:</p> <ul> <li><code>efficient_frontier</code>: <code>pypfopt.efficient_frontier.EfficientFrontier</code></li> <li><code>efficient_cdar</code>: <code>pypfopt.efficient_frontier.EfficientCDaR</code></li> <li><code>efficient_cvar</code>: <code>pypfopt.efficient_frontier.EfficientCVaR</code></li> <li><code>efficient_semivariance</code>: <code>pypfopt.efficient_frontier.EfficientSemivariance</code></li> <li><code>black_litterman</code> or <code>bl</code>: <code>pypfopt.black_litterman.BlackLittermanModel</code></li> <li><code>hrp</code>: <code>pypfopt.hierarchical_portfolio.HRPOpt</code></li> <li><code>cla</code>: <code>pypfopt.cla.CLA</code></li> </ul> <p>Black-litterman</p> <p>Black-Litterman model takes a Bayesian approach to asset allocation. Specifically, it combines a prior estimate of returns (for example, the market-implied returns) with views on certain assets, to produce a posterior estimate of expected returns. It will then meaningfully propagate views, taking into account the covariance with other assets. </p> absolute_views<pre><code>Optional[pandas.core.series.Series | Dict[str, float] | str] = None\n</code></pre> <p>A colleciton of K absolute views on a subset of assets, defaults to None. If set to <code>alpha_blocks</code>, it computes the \"alpha blocks views\". The alpha blocks implementation works with the Black-Litterman absolute views, where views are extracted from the median of weight range and magnitude calculated by multiplying the annualized volatility times \\(1.96\\). In probability and statistics, the 97.5th percentile point of the standard normal distribution is a number commonly used for statistical calculations. The approximate value of this number is 1.96, meaning that 95% of the area under a normal curve lies within approximately 1.96 standard deviations of the mean.</p> pi<pre><code>Optional[pandas.core.series.Series | numpy.ndarray] = None\n</code></pre> <p>Nx1 prior estimate of returns, defaults to None. If pi=\u201dmarket\u201d, calculate a market-implied prior (requires market_caps to be passed). If pi=\u201dequal\u201d, use an equal-weighted prior.</p> omega<pre><code>Optional[pandas.core.frame.DataFrame | numpy.ndarray | string] = None\n</code></pre> <p>KxK view uncertainty matrix (diagonal), defaults to None Can instead pass \u201cidzorek\u201d to use Idzorek\u2019s method (requires you to pass view_confidences). If omega=\u201ddefault\u201d or None, we set the uncertainty proportional to the variance.</p> view_confidences<pre><code>Optional[pandas.core.series.Series | numpy.ndarray | List] = None\n</code></pre> <p>Kx1 vector of percentage view confidences (between 0 and 1), required to compute omega via Idzorek\u2019s method.</p> tau<pre><code>Optional[float] = 0.05\n</code></pre> <p>the weight-on-views scalar (default is 0.05) risk_aversion (positive float, optional) \u2013 risk aversion parameter, defaults to 1.</p> market_caps<pre><code>Optional[pandas.core.series.Series | numpy.ndarray] = None\n</code></pre> <p>Market caps for the assets, required if pi=\u201dmarket\u201d</p> risk_free_rate<pre><code>Optional[float] = 0.02\n</code></pre> <p>Risk-free rate of borrowing/lending, defaults to 0.02. The period of the risk-free rate should correspond to the frequency of expected returns.</p> expected_returns<pre><code>Optional[str] = \"mean_historical_return\"\n</code></pre> <p>Specify the expected returns. <code>expected_returns</code> can be an array, an attribute of <code>pypfopt.expected_returns</code>, a function, or one of the following options:</p> <ul> <li><code>mean_historical_return</code>: <code>pypfopt.expected_returns.mean_historical_return</code></li> <li><code>ema_historical_return</code>: <code>pypfopt.expected_returns.ema_historical_return</code></li> <li><code>capm_return</code>: <code>pypfopt.expected_returns.capm_return</code></li> <li><code>bl_returns</code>: <code>pypfopt.black_litterman.BlackLittermanModel.bl_returns</code></li> </ul> cov_matrix<pre><code>Optional[str] = \"ledoit_wolf\"\n</code></pre> <p>Specify the covariance matrix. <code>cov_matrix</code> can be an array, an attribute of <code>pypfopt.risk_models</code>, a function, or one of the following options:</p> <ul> <li><code>sample_cov</code>: pypfopt.risk_models.sample_cov</li> <li><code>semicovariance</code> or <code>semivariance</code>: pypfopt.risk_models.semicovariance</li> <li><code>exp_cov</code>: pypfopt.risk_models.exp_cov</li> <li><code>ledoit_wolf</code>: <code>pypfopt.risk_models.CovarianceShrinkage.ledoit_wolf</code> with <code>constant_variance</code> as shrinkage factor</li> <li><code>ledoit_wolf_single_factor</code>: <code>pypfopt.risk_models.CovarianceShrinkage.ledoit_wolf</code> with <code>single_factor</code> as shrinkage factor</li> <li><code>ledoit_wolf_constant_correlation</code>: <code>pypfopt.risk_models.CovarianceShrinkage.ledoit_wolf</code> with <code>constant_correlation</code> as shrinkage factor</li> <li><code>oracle_approximating</code>: <code>pypfopt.risk_models.CovarianceShrinkage.ledoit_wolf</code> with <code>oracle_approximating</code> as shrinkage factor</li> </ul> weight_bounds<pre><code>Optional[Tuple[float, float]] = (-1, 1)\n</code></pre> <p>Minimum and maximum weight of each asset or single min/max pair if all identical, defaults to (-1, 1). If <code>weight_bounds=(-1, 1)</code>, allows short positions.</p> target_return<pre><code>float\n</code></pre> <p>The desired return of the resulting portfolio.</p> market_neutral<pre><code>Optional[bool] = False\n</code></pre> <p>whether the portfolio should be market neutral (weights sum to zero), defaults to False. Requires negative lower weight bound.</p> target_volatility<pre><code>float\n</code></pre> <p>The desired maximum volatility of the resulting portfolio.</p> risk_aversion<pre><code>Optional[int] = 1\n</code></pre> <p>Risk aversion parameter (must be greater than 0), defaults to 1.</p> n_asset_constraints<pre><code>Optional[List[Type]] = None\n</code></pre> <p>Number of assets in the portfolio constraints. Cardinality constraints are not convex, making them difficult to implement. However, we can treat it as a mixed-integer program and solve (provided you have access to a solver). for small problems with less than 1000 variables and constraints, you can use the community version of CPLEX available in python <code>pip install cplex</code>.</p> <p><code>n_asset_constraints</code></p> <p>This functionnality is still work in progress, as it requires external capabilities (<code>cplex</code>).</p>"},{"location":"api/strategy/portfolio_construction/optimization/#returns","title":"Returns","text":"<p><code>OrderedDict</code>, optimized weights.</p>"},{"location":"api/strategy/portfolio_construction/optimization/#example-optimizer","title":"Example Optimizer","text":"<p>Example Optimizer<p>Portfolio construction, which involves optimizing the allocation of assets within a portfolio, can be a complex and nuanced process. We have developed a method that allows for greater flexibility and experimentation in the portfolio optimization process. This approach enables the exploration of a wide range of potential portfolio compositions, and the example provided illustrates this method applied from the initial stages of portfolio construction:</p> <ul> <li>A mapping table, <code>mapping_table</code>, has been defined to specify the group membership of each investable stocks</li> <li>The base model is set <code>mvo</code>, the Mean-Variance Optimization from the pypfopt library, with the appropriate return and risk models</li> <li>The weight bounds parameter, <code>weight_bounds</code> is set to <code>(-1, 1)</code>, which serves as the first constraint by limiting the minimum and maximum weight of each asset in portfolios that allow short positions</li> <li>Additionally, new constraints are introduced to the optimization problem in the form of convex inequalities, which ensure that long positions do not exceed 10% and short positions do not fall below -10%</li> </ul> <pre><code>from opendesk import Strategy\nstrategy = Strategy(\nsteps=steps, \ntopdown=True,\nmapping_table=mapping_table # (3)\n)\nstrategy.fit(sector_prices).estimate(sum) # (1)\nweights = strategy.portfolio(stock_prices).optimize( # (2)\ntarget=\"min_volatility\"\nweight_bounds=(-1, 1) # (4)\nconstraints=[\nlambda w: w &lt;=  .1, \nlambda w: w &gt;= -.1\n] # (5)\n)\n</code></pre> <ol> <li>pandas.DataFrame object, with specifiy the variation of sector returns over time.</li> <li>pandas.DataFrame object, with specifiy the variation of stock prices over time.</li> <li>Mapping table where stock ticker/id are keys and sector name are values.</li> <li><code>weight_bounds</code> parameter serves as a constraint by limiting the minimum and maximum weight of each asset in portfolios. Because it ranges from <code>-1</code> to <code>1</code>, it allows Long and Shorts.</li> <li>Users can add new constraints in a form of lambda function as the user need to the optimization problem. This constraint must satisfy DCP rules, i.e be either a linear equality constraint or convex inequality constraint.</li> </ol> </p>"},{"location":"contributing/","title":"Developement Guildeline","text":""},{"location":"contributing/development_guidelines/","title":"Development Guidelines","text":""},{"location":"contributing/development_guidelines/#abstract-classes-abcs","title":"Abstract Classes (ABCs)","text":"<p>An abstract class can be considered as a blueprint for other classes. It allows you to create a set of methods that must be created within any child classes built from the abstract class. </p> <ul> <li>A class which contains one or more abstract methods is called an abstract class. </li> <li>An abstract method is a method that has a declaration but does not have an implementation. </li> <li>While we are designing large functional units we use an abstract class.</li> <li>When we want to provide a common interface for different implementations of a component, we use an abstract class.\u00a0</li> </ul>"},{"location":"contributing/development_guidelines/#why-are-abcs-useful","title":"Why are ABCs useful?","text":"<p>By defining an abstract base class, you can define a common Application Program Interface(API) for a set of subclasses. This capability is especially useful in situations where a third-party is going to provide implementations, such as with plugins, but can also help you when working in a large team or with a large code-base where keeping all classes in your mind is difficult or not possible.</p>"},{"location":"contributing/development_guidelines/#how-abcs-work","title":"How ABCs work?","text":"<p>ABC</p> <p>Find more information about abc \u2014 Abstract Base Classes.</p> <p>By default, Python does not provide abstract classes. Python comes with a module that provides the base for defining Abstract Base classes (ABC) and that module name is <code>ABC</code>. <code>ABC</code> works by decorating methods of the base class as abstract and then registering concrete classes as implementations of the abstract base. A method becomes abstract when decorated with the keyword <code>@abstractmethod</code>. </p>"},{"location":"contributing/development_guidelines/#base-score-model","title":"Base Score Model","text":"<p>In order to ensure consistency and ease of use, all blocks must adhere to the <code>BaseScoreModel</code> abstract base class. </p> <pre><code>from typing import Dict\nimport pandas as pd\nfrom abc import ABC, abstractmethod\nclass BaseScoreModel(ABC):\n@abstractmethod\ndef processing(\nself, X: pd.DataFrame, **kwargs) -&gt; \"BaseScoreModel\":\nself.processing_output: pd.DataFrame\nreturn self\n@abstractmethod\ndef ranking(self) -&gt; \"BaseScoreModel\":\nself.ranking_output: pd.DataFrame\nreturn self\n@abstractmethod\ndef scoring(self) -&gt; \"BaseScoreModel\":\nself.scoring_output: pd.DataFrame\nself.strategy_output: Dict\nreturn self\n</code></pre> <p>This set of rules allows us to seamlessly integrate blocks and reduces friction. As a result, the code becomes more readable, easier to debug, and simpler to contribute to. It also significantly decreases the amount of code required to access information.</p>"},{"location":"contributing/development_guidelines/#step-1-inheritance","title":"Step 1: Inheritance","text":"<p>Blocks must inherite from <code>BaseScoreModel</code>:</p> <pre><code>from opendesk import BaseScoreModel\nclass MyBlock(BaseScoreModel)\n...\n</code></pre>"},{"location":"contributing/development_guidelines/#step-2-initialize-parameters","title":"Step 2: Initialize Parameters","text":"<p>Parameters should be initialized at the <code>__init__</code> section of the class. It also means that model data should NOT be passed at this stage (see step 5 below):</p> <pre><code>from opendesk import BaseScoreModel\nclass MyBlock(BaseScoreModel)\ndef __init__(self, param_1, param_2, param_n):\nself.param_1 = param_1\nself.param_2 = param_2\nself.param_n = param_n\n</code></pre>"},{"location":"contributing/development_guidelines/#step-3-prs","title":"Step 3: PRS","text":"<p>Blocks must follow the \"PRS\" schema, which is \"Processing\" data, \"Ranking\" results, \"Scoring\" output. All classes must have the mandatory three functions:</p> <ul> <li><code>processing()</code></li> <li><code>ranking()</code></li> <li><code>scoring()</code></li> </ul> <pre><code>from opendesk import BaseScoreModel\nclass MyBlock(BaseScoreModel)\ndef __init__(self, param_1, param_2, param_n):\nself.param_1 = param_1\nself.param_2 = param_2\nself.param_n = param_n\ndef processing(self)\n...\ndef ranking(self)\n...\ndef scoring(self)\n...\n</code></pre>"},{"location":"contributing/development_guidelines/#step-4-fluent-interface","title":"Step 4: Fluent Interface","text":"<p>Each of the above three functions must return the instance object <code>self</code> on which the method was called:</p> <pre><code>from opendesk import BaseScoreModel\nclass MyBlock(BaseScoreModel)\ndef __init__(self, param_1, param_2, param_n):\nself.param_1 = param_1\nself.param_2 = param_2\nself.param_n = param_n\ndef processing(self) -&gt; \"MyBlock\":\nreturn self\ndef ranking(self) -&gt; \"MyBlock\":\nreturn self\ndef scoring(self) -&gt; \"MyBlock\":\nreturn self\n</code></pre> <p>Fluent interface</p> <p>In object-oriented programming, returning self from a method allows for the implementation of a fluent interface, where methods can be cascaded i a single statement.</p> <p>In object-oriented programming, returning <code>self</code> from a method can be useful for several reasons. One common use case is to create a fluent interface, which is an API design style that allows method calls to be chained together in a single statement. This can make the code more readable and concise, as it eliminates the need to create intermediate variables to store the results of intermediate method calls. For example, with a fluent interface, this code could be written as <code>results = SomeClass().method1().method2().method3()</code>. </p>"},{"location":"contributing/development_guidelines/#step-5-processing-parameters","title":"Step 5: Processing Parameters","text":"<p>The function <code>processing()</code> must accept a market price <code>pandas.DataFrame</code> time-series object and can accept additional dataset (within <code>**kwargs</code>):</p> <pre><code>import pandas as pd\nfrom opendesk import BaseScoreModel\nclass MyBlock(BaseScoreModel)\ndef __init__(self, param_1, param_2, param_n):\nself.param_1 = param_1\nself.param_2 = param_2\nself.param_n = param_n\ndef processing(self, X: pd.DataFrame, **kwargs) -&gt; \"MyBlock\":\nreturn self\ndef ranking(self) -&gt; \"MyBlock\":\nreturn self\ndef scoring(self) -&gt; \"MyBlock\":\nreturn self\n</code></pre>"},{"location":"contributing/development_guidelines/#step-6-set-attributes","title":"Step 6: Set Attributes","text":"<p>Functions <code>processing()</code>, <code>ranking()</code> and <code>scoring()</code> must set attributes <code>processing_output</code>, <code>ranking_output</code> and <code>scoring_output</code> as <code>pandas.DataFrame</code> object, respectively:</p> <pre><code>import pandas as pd\nfrom opendesk import BaseScoreModel\nclass MyBlock(BaseScoreModel)\ndef __init__(self, param_1, param_2, param_n):\nself.param_1 = param_1\nself.param_2 = param_2\nself.param_n = param_n\ndef processing(self, X: pd.DataFrame, **kwargs) -&gt; \"MyBlock\":\nself.processing_output: pd.DataFrame\nreturn self\ndef ranking(self) -&gt; \"MyBlock\":\nself.ranking_output: pd.DataFrame\nreturn self\ndef scoring(self) -&gt; \"MyBlock\":\nself.scoring_output: pd.DataFrame\nreturn self\n</code></pre>"},{"location":"contributing/development_guidelines/#step-7-strategy-output","title":"Step 7: Strategy Output","text":"<p>The function <code>scoring()</code> must set attribute <code>strategy_output</code> as dictionnary object, representing the final scores of the model:</p> <pre><code>from typing import Dict\nimport pandas as pd\nfrom opendesk import BaseScoreModel\nclass MyBlock(BaseScoreModel)\ndef __init__(self, param_1, param_2, param_n):\nself.param_1 = param_1\nself.param_2 = param_2\nself.param_n = param_n\ndef processing(self, X: pd.DataFrame, **kwargs) -&gt; \"MyBlock\":\nself.processing_output: pd.DataFrame\nreturn self\ndef ranking(self) -&gt; \"MyBlock\":\nself.ranking_output: pd.DataFrame\nreturn self\ndef scoring(self) -&gt; \"MyBlock\":\nself.scoring_output: pd.DataFrame\nself.strategy_output: Dict\nreturn self\n</code></pre>"},{"location":"contributing/philosophy/","title":"Philosophy","text":"<p>The engineering and research process in the context of investment strategies involves several key stages.</p> <ol> <li>Research phase: In this phase, the focus is on gathering and analyzing data to inform the development of a quantitative model or algorithm. This involves collecting data from various sources, such as financial statements, market trends, and economic indicators, and using statistical and mathematical techniques to analyze and interpret the data.</li> <li>Strategy implementation: Once the research phase is complete, the next step is to implement the quantitative model or algorithm. </li> <li>Model Optimization: After the initial implementation of the quantitative model, it is important to continuously review and optimize it to ensure that it is accurately predicting outcomes and providing valuable insights. This may involve  adjusting the model's parameters or adding additional data sources to improve its performance.</li> <li>Backtesting: Before implementing a quantitative model in a live trading environment, it is often useful to backtest it to see how it would have performed in the past. This can help to identify any potential weaknesses or areas for improvement in the model.</li> <li>Target and invested portfolio: In the context of quantitative analysis, the target portfolio refers to the desired mix of investments that the model is intended to identify and select, while the invested portfolio refers to the actual mix of investments that has been chosen based on the model's predictions.</li> <li>Risk monitoring: It is important to continuously monitor the portfolio for any potential risks and to take appropriate action to mitigate those risks. This involves regularly reviewing the portfolio and making adjustments as needed, as well as staying informed about market trends and economic conditions.</li> </ol> <pre><code>sequenceDiagram\n  Database-&gt;&gt;Blocks: Feed Strategy\n  Blocks-&gt;&gt;Portfolio: Compute Exposures\n  Portfolio-&gt;&gt;Backtest: Weights\n  loop Objectives &amp; Constraints\n        Backtest-&gt;&gt;Portfolio: Performance\n    end</code></pre> <p>Overall, the engineering and research process in the context of quantitative analysis involves a systematic approach to collecting and analyzing data, developing and implementing a quantitative model or algorithm, and continuously reviewing and optimizing it to ensure that it is providing accurate and valuable insights.</p>"},{"location":"data_workflow/","title":"Index","text":""},{"location":"data_workflow/#data-workflow","title":"Data workflow","text":"<p>What is AWS CloudFormation?</p> <p>Documentation available on What is AWS CloudFormation? - AWS CloudFormation</p> <pre><code>sequenceDiagram\n    Data Provider-&gt;&gt;AWS S3: File Upload\n    AWS S3--&gt;&gt;AWS Lambda: Trigger Lambda Function\n    AWS Lambda--&gt;&gt;AWS DynamoDB: Metadata Store\n    AWS Lambda--&gt;&gt;AWS S3: Extract File Uploaded\n    AWS S3--&gt;&gt;AWS Lambda: Trigger Lambda Function\n    AWS Lambda--&gt;&gt;AWS S3: HTML Upload\n    AWS S3-&gt;&gt;Terminal: Online Access</code></pre> <p>AWS CloudFormation is a service that helps you model and set up your AWS  resources so that you can spend less time managing those resources and more  time focusing on your applications that run in AWS. You create a template that  describes all the AWS resources that you want (like Amazon EC2 instances or  Amazon RDS DB instances), and CloudFormation takes care of provisioning and  configuring those resources for you. You don't need to individually create and configure AWS resources and figure out what's dependent on what; CloudFormation  handles that. The following scenarios demonstrate how CloudFormation can help.</p>"},{"location":"dataset/","title":"Transformed Dataset","text":""},{"location":"dataset/#coming-soon","title":"Coming Soon!","text":""},{"location":"getting_started/","title":"Opendesk","text":"<p>This Get Started guide explains how Opendesk works, how to install Opendesk on your preferred operating system, and how to create your first Opendesk strategy!</p> <p></p> <p>Installation helps you set up your virtual environment and walks you through installing Opendesk on Windows, macOS, and Linux. Regardless of which package management tool and OS you're using, we recommend running the commands on this page in a virtual environment.</p> <p></p> <p>Main concepts introduces you to Opendesk's base model and development flow. You'll learn what makes Opendesk the most powerful way to build systematic strategies, including the ability to customize existing setup. </p> <p></p> <p>Create Blocks following the three-step process outlined in Opendesk's core guidelines: process raw data, rank and compare these results, create and map a sequence of scores.</p> <p></p> <p>Optimize Blocks, which is the process of selecting the optimal mix of assets in a portfolio, with respect to the alpha scores, in order to maximize returns while minimizing risk. This method has been implemented to streamline the process of optimization and facilitate the integration of backtesting.</p> <p></p> <p>Backtest Blocks using Opendesk's vectorbt wrapper to test one or multiple strategies at once, optimizes portfolio and provides access to the full range of functionality offered by the vectorbt ecosystem.</p>"},{"location":"getting_started/backtest_blocks/","title":"Backtest Blocks","text":""},{"location":"getting_started/backtest_blocks/#open-source-resources","title":"Open-Source Resources","text":"<p>This platform aggregates a range of state-of-the-art tools and libraries and facilitates their automation through a unified interface. This allows users to easily access and utilize a wide range of advanced resources for their research or development efforts, streamlining the process of utilizing cutting-edge technologies.</p> <p>There is a wide range of options available for Python when it comes to selecting a backtesting framework. A comprehensive list of these libraries can be found on the GitHub page titled awesome-quant. We initially experimented with one called Backtesting and then subsequently tested a few others:</p> <ul> <li>Backtesting</li> <li>bt </li> <li>vectorbt</li> <li>backtrader </li> </ul> <p>For the purpose of backtesting, vectorbt can be considered as a suitable initial choice for integrating our model pipeline due to its fast performance and ease of use. It also actively managed and a more advance version has been created: vectorbt pro.</p>"},{"location":"getting_started/backtest_blocks/#integration","title":"Integration","text":"<p>vectorbt is a Python package for quantitative analysis that takes a novel approach to backtesting: it operates entirely on pandas and NumPy objects, and is accelerated by Numba to analyze any data at speed and scale. This allows for testing of many thousands of strategies in seconds.</p> <p>Vectorbt is a powerful tool that combines the functionality of a fast backtester with advanced data analysis capabilities. It enables users to analyze and evaluate a wide range of trading options, instruments, and time periods with ease, enabling them to identify patterns and optimize their strategy. It allows users to explore and understand complex phenomena in trading data, providing them with valuable insights that can inform their decision-making and potentially give them an informational advantage in the market.</p> <p>It utilizes a vectorized representation of trading strategies in order to optimize performance. This representation involves storing multiple strategy instances in a single multi-dimensional array, as opposed to the traditional object-oriented programming (OOP) approach of representing strategies as classes and data structures. The vectorized representation used by vectorbt allows for more efficient processing and comparison of strategies, and can particularly improve the speed of analysis when dealing with quadratic programming problems.</p> <p>Numba is a just-in-time (JIT) compiler for Python that is designed to improve the performance of Python code. It does this by compiling Python code to native machine instructions, which can be executed much faster than the interpreted code that is normally used in Python. Numba works by decorating functions or methods with a special <code>@jit</code> decorator, which tells the Numba compiler to compile the function for faster execution. Numba can be used to speed up code that makes heavy use of Python's scientific and numerical libraries, such as NumPy and SciPy, as well as code that performs CPU-bound operations. In addition to its JIT compiler, Numba also provides support for parallel programming through its <code>@vectorize</code> and <code>@guvectorize</code> decorators, which can be used to parallelize the execution of certain types of functions.</p>"},{"location":"getting_started/backtest_blocks/#step-by-step-example","title":"Step-by-Step Example","text":"<p>The <code>backtest</code> classmethod within the <code>Strategy</code> class utilizes the vectorbt library to test and evaluate the performance of a given strategy in the context of alpha blocks methodology.</p> <p>In the field of software engineering, it is recommended that a function be designed in a manner that promotes testability by adhering to the principles of cohesion. Specifically, this entails limiting the function's scope to a single, well-defined task, as well as minimizing the number of arguments it accepts. </p> <p>In cases where a function is required to operate on multiple arguments, it is suggested to employ techniques such as encapsulation by utilizing higher-level objects to group the arguments together. This allows for more robust and maintainable codebase.</p> <p>This is why we created <code>BacktestConfig</code>, a dataclass object, which encapsultes required arguments to create exposure and build portfolio at each rebalancing period.</p>"},{"location":"getting_started/backtest_blocks/#step-1-backtestconfig","title":"Step 1: BacktestConfig","text":"<p>The <code>backtest</code> method is initalized through the <code>BacktestConfig</code>, which facilitates feature integration. For example, variables <code>universe</code>, <code>model_data</code>, <code>steps</code>, <code>topdown</code> and <code>mapping_table</code> are set to match your requirement. All other variables are pre-set.</p> <pre><code>from opendesk.backtest import BacktestConfig\nconfig = BacktestConfig(\nsteps=[(\n\"my_block\", \nMyBlock, \n{\"mapping_score\": mapping_score}, \n{\"price_earnings\": price_earnings}\n)],     \nuniverse=stock_prices, \nmodel_data=model_data, \ntopdown=True, \nmapping_table=mapping_table,\nportfolio='optimize',\nbacktest_backup=\"discrete_allocation\"\n)\n</code></pre> <p>The backtest implementation initializes, fits and estimates exposures using the <code>fit()</code> and the <code>estimate()</code> methods. Then, it optimizes the portfolio at the stock level using the <code>portfolio()</code> and <code>optimize()</code> methods and finds weights that align with the desired level of risk. </p> <p>The strategy is rebalanced on a monthly basis, and the <code>discrete_allocation()</code> method is used as a fallback in the event that the optimizer is unable to deliver feasible weights.</p>"},{"location":"getting_started/backtest_blocks/#step-2-backtest","title":"Step 2: Backtest","text":"<p>The output is a <code>PortfolioOptimizer</code> object, which allows users to leverage the entire vectorbtpro ecosystem.</p> <pre><code>$ backtest = Strategy.backtest(config)\n$ backtest.stats()\n&lt;span style=\"color: grey;\"&gt;Start                                 2019-01-02 00:00:00\nEnd                                   2022-12-30 00:00:00\nPeriod                                 1008 days 00:00:00\nStart Value                                         100.0\nEnd Value                                      152.092993\nTotal Return [%]                                52.092993\nBenchmark Return [%]                            62.173178\nMax Gross Exposure [%]                          31.819902\nTotal Fees Paid                                       0.0\nMax Drawdown [%]                                13.822639\nMax Drawdown Duration                   320 days 00:00:00\nTotal Trades                                          391\nTotal Closed Trades                                   372\nTotal Open Trades                                      19\nOpen Trade PnL                                  14.614093\nWin Rate [%]                                    54.301075\nBest Trade [%]                                 122.866679\nWorst Trade [%]                               -164.309231\nAvg Winning Trade [%]                           22.115064\nAvg Losing Trade [%]                            -19.44764\nAvg Winning Trade Duration    158 days 21:51:40.990099010\nAvg Losing Trade Duration     143 days 02:49:24.705882354\nProfit Factor                                    1.417933\nExpectancy                                        0.10075\nSharpe Ratio                                     0.942305\nCalmar Ratio                                     0.799575\nOmega Ratio                                      1.179846\nSortino Ratio                                    1.431216\nName: group, dtype: object\n&lt;/span&gt;\n</code></pre>"},{"location":"getting_started/create_blocks/","title":"Create Blocks","text":"<p>Blocks are classes, which follow Opendesk's Development Guidelines, to ensure consistency and ease of use. Blocks must follow the \"PRS\" schema, which is \"Processing\" data, \"Ranking\" results, \"Scoring\" output. All classes must have the below three functions:</p> <ul> <li><code>processing()</code></li> <li><code>ranking()</code></li> <li><code>scoring()</code></li> </ul>"},{"location":"getting_started/create_blocks/#step-by-step-example","title":"Step-by-Step Example","text":"<p>Dependencies</p> <p>For this step-by-step example, we are using the following dependencies: <pre><code>import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n</code></pre></p> <p>In this example, we want to create a valuation level strategy. This rule-based strategy implies exposing our portfolio to directional bets, depending on both the level of the price-earning ratio (PE) time-series and the cross-sectional comparison between assets.</p> <ol> <li>In the processing function, we will standardize features by removing the mean and scaling to unit variance. The standard score, \\(Z\\), of a sample \\(x\\) is calculated as \\(Z = \\frac{(x - \\mu)}{\\sigma}\\), where \\(\\mu\\) is the mean of the training samples and \\(\\sigma\\) is the standard deviation of the training samples.</li> <li>In the ranking function, we will rank in ascending order \\(PEs\\).</li> <li>In the scoring function, we will map top quantiles from both long and short sides. </li> </ol>"},{"location":"getting_started/create_blocks/#step-1-processing","title":"Step 1: Processing","text":"<p>To process data, we use the Scikit-Learn <code>StdandardScaler</code> transform. In practice, normalizing our time-series is as follow:</p> <pre><code>def processing(price_earnings: pd.DataFrame):\nprocessing_output = (\nStandardScaler()\n.set_output(transform=\"pandas\")\n.fit_transform(price_earnings)\n)\nreturn processing_output\n</code></pre> <p>Which returns a <code>pandas.DataFrame</code> object.</p>"},{"location":"getting_started/create_blocks/#step-2-ranking","title":"Step 2: Ranking","text":"<p>Then, the transformed results is ranked using <code>pandas.rank()</code> function:</p> <pre><code>def ranking():\nranking_output = (\nprocessing_output\n.iloc[-1]\n.rename(\"rank\")\n.rank(ascending=False)\n.sort_values()\n)\nreturn ranking_output\n</code></pre> <p>Which also returns a <code>pandas.DataFrame</code> object.</p>"},{"location":"getting_started/create_blocks/#step-3-scoring","title":"Step 3: Scoring","text":"<p>Finally, we produce the goal of this base score model is to produce quantile scores, as follow:</p> <pre><code>def scoring():\nquantiles = pd.qcut(ranking_output, q=5, labels=False)\nscoring_output = quantiles.map(mapping_score)\nreturn strategy_output\n</code></pre>"},{"location":"getting_started/create_blocks/#step-4-parameters","title":"Step 4: Parameters","text":"<p>Last, for reproductibility and ease, we aim to gather parameters in the <code>__init__</code> section:</p> <pre><code>from typing import Dict\nfrom opendesk.blueprints import BaseScoreModel\nclass MyBlock(BaseScoreModel):\ndef __init__(self, mapping_score: Dict)\nself.mapping_score = mapping_score\n</code></pre>"},{"location":"getting_started/create_blocks/#step-5-wrapping-things-up","title":"Step 5: Wrapping Things Up","text":"<pre><code>import pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom opendesk.blueprints import BaseScoreModel\nclass MyBlock(BaseScoreModel):\ndef __init__(self, mapping_score: Dict)\nself.mapping_score = mapping_score\ndef processing(self, price_earnings: pd.DataFrame) -&gt; \"MyBlock\":\nself.processing_output = (\nStandardScaler()\n.set_output(transform=\"pandas\")\n.fit_transform(price_earnings)\n)\nreturn self\ndef ranking(self) -&gt; \"MyBlock\":\nself.ranking_output = (\nprocessing_output\n.iloc[-1]\n.rename(\"rank\")\n.rank(ascending=False)\n.sort_values()\n)\nreturn self\ndef scoring(self) -&gt; \"MyBlock\":\nquantiles = pd.qcut(ranking_output, q=5, labels=False)\nself.scoring_output = quantiles.map(mapping_score)\nself.strategy_output = scoring_output.to_dict()\nreturn self\n</code></pre>"},{"location":"getting_started/installation/","title":"Installation","text":""},{"location":"getting_started/installation/#prerequisites","title":"Prerequisites","text":"<p>Before you get started, you're going to need a few things:</p> <ul> <li>Your favorite IDE or text editor</li> <li>Python 3.10</li> <li>PIP</li> <li>Install C++:</li> <li>MacOS: You need to install XCode Command Line Tools. They are required to compile some of Opendesk's Python dependencies during installation.</li> <li>Windows: For Windows users, download Visual Studio, with additional instructions.</li> </ul> <p>Python</p> <p>Opendesk is built on Python. Other dependencies, such as C++, are employed to improve the speed at which the program runs.</p>"},{"location":"getting_started/installation/#set-up-your-virtual-environment","title":"Set up your virtual environment","text":"<p>Regardless of which package management tool you're using, we recommend running the commands on this page in a virtual environment. This ensures that the dependencies pulled in for Opendesk don't impact any other Python projects you're working on.</p> <p>Below are a few tools you can use for environment management:</p> <ul> <li>pipenv</li> <li>poetry</li> <li>venv</li> <li>virtualenv</li> <li>conda</li> </ul>"},{"location":"getting_started/installation/#install-opendesk","title":"Install Opendesk","text":"<p>Troubleshooting</p> <p>If any of these methods don\u2019t work, please raise an issue with the \u2018packaging\u2019 label on GitHub.</p> <p>prior to using the below commands, you may need to follow the installation instructions for cvxopt and cvxpy).</p>"},{"location":"getting_started/installation/#with-pip","title":"With Pip recommended","text":"<p>Installation can be done via pip:</p> <pre><code>$ pip install opendesk\n---&gt; 100%\n</code></pre>"},{"location":"getting_started/installation/#with-poetry","title":"With Poetry recommended","text":"<p>Poetry is a relatively new kid on the block but has gained traction due to its ease of use and how it resolves several issues over Conda.</p> <p>Poetry will set up your virtual environment and install all required packages with one command <code>poetry install</code>. In addition, it can build installable packages with <code>poetry build</code>, which can be installed by pip. It uses the PEP Standard pyproject.toml file to define dependencies and build options for other tools (e.g. pytest arguments).</p> <p>It respects <code>.python_version</code> files, uses a lock file to define specific versions of packages and pip to install them - thus, you have reproducible environments installed quickly.</p> <p>The only downside is the slight learning curve in changing your workflow.</p> <pre><code>$ poetry add opendesk\n---&gt; 100%\n</code></pre>"},{"location":"getting_started/installation/#with-conda","title":"With Conda","text":"<p>If you don't have Anaconda install yet, follow the steps provided on the Anaconda installation page.</p> <pre><code>$ conda install opendesk\n---&gt; 100%\n</code></pre>"},{"location":"getting_started/installation/#with-git","title":"With Git","text":"<p>The alternative is to clone the project:</p> <pre><code>$ git clone https://github.com/ActurialCapital/xyz.git\n$ cd myproject\n$ python setup.py install\n</code></pre>"},{"location":"getting_started/installation/#further-reading","title":"Further Reading","text":"<p>A pretty good guide/supporting argument to the recommended setup - My Python Development Environment, 2020 Edition - Jacob Kaplan-Moss.</p>"},{"location":"getting_started/main_concepts/","title":"Main Concepts","text":"<p>Using OpenDesk is easy. First, come up with an idea and make sure it can be organized in a clear and concise way. Then, build one or more \"blocks\" that bring this idea to life, following OpenDesk's core guidelines. Finally, create a portfolio and test it out through backtesting to see how it performs.</p> <p>Building alpha blocks is a methodology which involves a sequential application of multiple, modular strategies. These blocks are rule-based and can be easily modified, added, or removed from the sequence. Each block returns a score, and the overall approach is designed to facilitate the integration of research and the efficient testing and optimization of strategies and Machine Learning overlaye, while also allowing for the risk-efficient combination of different sources of alpha. The building blocks approach is a comprehensive yet extensible method for data analysis and financial strategy development. </p>"},{"location":"getting_started/main_concepts/#features","title":"Features","text":"<ul> <li> Portfolio strategy first: Opendesk focuses on portfolio strategy development</li> <li> Fast: Opendesk system has been designed to achieve superior performance in the development, implementation, and evaluation of strategies, built with NumPy and Pandas</li> <li> Fast to code: Created through fluent interface, which is an API design style that allows method calls to be chained together in a single statement. This make Opendesk's code more readable and concise, as it eliminates the need to create intermediate variables to store the results of temporary method calls.</li> <li> Intuitive: Develop a plan, organize your ideas, conduct research, implement strategies based on the findings, and test various parameters to determine their impact in a single session.</li> <li> Easy: Opendesk user interface has been optimized for simplicity and ease of use, reducing the need for extensive training or consultation of documentation.</li> <li> <p> Batteries included: </p> <ul> <li>Quantitative data pre-processing</li> <li>ML and scientific computing</li> <li>Large and diversified model library</li> <li>Flexible portfolio construction</li> <li>Fast Backtesting capabilities</li> <li>AWS CloudFormation service integrated</li> <li>FastAPI web framework built-in</li> </ul> </li> </ul> <p>Our platform integrates a diverse set of state-of-the-art tools and libraries, including those for machine learning, data analysis, and scientific computing. These resources are carefully curated to ensure that they represent the most advanced and effective technologies currently available.</p>"},{"location":"getting_started/main_concepts/#alpha-blocks","title":"Alpha blocks","text":"<p>The alpha block sequence is a modular approach to creating investment strategies that involves breaking down the code pipeline into individual units, or \"blocks,\" that can be easily replaced or removed. Each block is designed to function as a rule-based strategy, contributing to the overall views and confidence assigned to a particular asset. This approach, which was first proposed by Kowalski1 (1979), involves arranging blocks of code in a specific sequence in order to generate an output.The modular design of building sequences allows for easy expansion and integration of alpha sources. This approach has already been well-established in computer science and software engineering (Bass, Clements, &amp; Kazman, 20032). It is escpecially powerful for integrating research, as it allows for the combination of alpha sources in a risk-sensitive manner and leads towards efficient backtesting and optimization.</p> <p>Did you know?</p> <p>There are several scientific references that support the use of modular approaches and building blocks in investment strategy development. For example, in their paper \"Modular Investment Strategies,\" published in the Journal of Financial Economics3, K. G. Rouwenhorst and G. S. Wu discuss the benefits of using a modular approach to constructing investment strategies.  They argue that this approach allows for better risk management, as it allows for the independent testing and evaluation of each component of the strategy. Another scientific reference that supports the use of building blocks in investment strategy development is the paper \"Building Investment Strategies with Building Blocks,\" published in the Journal of Portfolio Management4. In this paper, the authors discuss the benefits of using a building block approach, including the ability to easily incorporate new research and the flexibility to adjust the strategy as market conditions change.</p>"},{"location":"getting_started/main_concepts/#scores","title":"Scores","text":"<p>Scores are numerical values that are used to evaluate or rank various options or candidates. In order to generalize the results, a score is generated for each block of data, which serves as a signal to adjust the level of exposure to a particular asset or group of assets:</p> <ul> <li>Scores allow for flexibility: By calculating scores, you can easily adjust the relative importance of different factors and criteria, or even add or remove factors as needed. This flexibility can be useful when an your goals or circumstances change.</li> <li> <p>Scores can be more easily customized: Scores can be customized to reflect your specific goals, risk tolerance, and other factors. This can make it easier for you to create portfolios that are tailored to your individual needs and preferences.</p> </li> <li> <p>Scores can help to incorporate subjective factors: Some investment decisions involve subjective judgment, such as whether a company's management team is strong or whether a particular market is likely to experience significant growth in the near future. Scores can be used to incorporate these subjective factors in a systematic way.</p> </li> <li> <p>Scores can be used to compare investments on a common scale: By assigning scores to different investments, you can more easily compare them to one another on a common scale. This can make it easier to identify the most attractive candidates for inclusion in a portfolio.</p> </li> <li> <p>Scores extend optimization processes: The optimization process can use these scores to select the investment portfolio that maximizes returns while minimizing risk, subject to any constraints or objectives that have been defined.</p> </li> </ul>"},{"location":"getting_started/main_concepts/#model-design","title":"Model Design","text":"<p>The building blocks approach involves sequentially executing multiple steps on a dataset, allowing for the modification of steps, step parameters, and the order in which they are executed. Each block represents a modular, rule-based strategy that can be removed or replaced in the sequence. These blocks return a score for each feature. This approach is designed to facilitate the integration of alpha sources, while also being easily extensible.</p> <pre><code>graph LR\n  id1[(Database)] --&gt; B[Strategy];\n  B --&gt; C[Block A] &amp; D[Block B] &amp; E[Block N];\n  C --&gt; I[Exposures];\n  D --&gt; I;\n  E --&gt; I;</code></pre>"},{"location":"getting_started/main_concepts/#speed","title":"Speed","text":""},{"location":"getting_started/main_concepts/#multiprocessing","title":"Multiprocessing","text":"<p>Using multiprocessing can be an efficient way to integrate the blocks. Multiprocessing refers to the ability of a computer to execute multiple processes concurrently, which can be useful for completing tasks more quickly. By utilizing multiprocessing to integrate each individual units, it is possible to speed up the process of constructing code pipelines and evaluating the performance of investment strategies or models.</p> <pre><code>graph LR\n  id1[(Database)] --&gt; B[Strategy];\n  B --&gt; C[Block A] &amp; D[Block B] &amp; E[Block N];\n  subgraph Multiprocessing\n  C --- F[/Score A/];\n  D --- G[/Score B/];\n  E --- H[/Score N/];\n  end\n  F --&gt; I[Exposures];\n  G --&gt; I;\n  H --&gt;I;</code></pre> AdvantagesLimitations <ol> <li>Improved performance: Multiprocessing can improve the performance of your program by allowing it to utilize multiple CPU cores, which can speed up the execution of CPU-bound and multithreaded programs.</li> <li>Better utilization of resources: Multiprocessing allows you to make use of all available CPU cores, which can be especially useful when running programs on a machine with multiple cores or on a multi-core server.</li> <li>Easier parallelization: Python's Joblib module provides a high-level interface for creating and managing processes, making it easier to write parallelized programs. Joblib provides a  simple helper class to write parallel for loops using multiprocessing. The core idea is to write the code to be executed as a generator expression, and convert it to parallel computing.</li> </ol> <p>However, it is important to note that the efficiency of using multiprocessing will depend on the specific characteristics of the blocks and the hardware being used. It may be necessary to carefully analyze the performance of the code and the available resources in order to determine the most appropriate approach for integrating the blocks in the sequence.</p> <p>In the above example, 5 blocks (processes) were executed concurrently using backend <code>LokyBackend</code> with 8 concurrent workers5, each operating on a dataset of 1000 samples times 20 sectors.</p> <p>The elapsed time for this operation was 1.2 seconds.</p> <pre><code>$ strategy.fit(df).estimate(sum)\n&lt;span style=\"color: yellow;\"&gt;[Parallel(n_jobs=-1)]:   &lt;/span&gt;&lt;span style=\"color: lime;\"&gt;Done 1 tasks&lt;/span&gt;\n&lt;span style=\"color: yellow;\"&gt;[Parallel(n_jobs=-1)]:   &lt;/span&gt;&lt;span style=\"color: lime;\"&gt;Done 2 out of 5&lt;/span&gt;    \n&lt;span style=\"color: yellow;\"&gt;[Parallel(n_jobs=-1)]:   &lt;/span&gt;&lt;span style=\"color: lime;\"&gt;Done 3 out of 5&lt;/span&gt;\n&lt;span style=\"color: yellow;\"&gt;[Parallel(n_jobs=-1)]:   &lt;/span&gt;&lt;span style=\"color: lime;\"&gt;Done 4 out of 5&lt;/span&gt; \n&lt;span style=\"color: yellow;\"&gt;[Parallel(n_jobs=-1)]:   &lt;/span&gt;&lt;span style=\"color: lime;\"&gt;Done 5 out of 5 | elapsed: 1.2s finished&lt;/span&gt;\n</code></pre>"},{"location":"getting_started/main_concepts/#polars","title":"Polars","text":"<p>In some instances, we use polars, a lightning-fast DataFrame library for Rust and Python. Polars is written in Rust, uncompromising in its choices to provide a feature-complete DataFrame API to the Rust ecosystem. We use it as a DataFrame library for your data models. Polars is built upon the safe Arrow2 implementation of the Apache Arrow specification, enabling efficient resource use and processing performance. By doing so it also integrates seamlessly with other tools in the Arrow ecosystem.</p> <p>For example, the <code>apply</code> function is used to aggregate scores from different blocks. Here, we tested 10000 rows x 10 columns:</p> <p>In pandas: <pre><code>df.apply(sum, axis=1).rename(\"sum\")\n</code></pre> Time to apply with pandas: <code>3.211 seconds</code></p> <p>In polars: <pre><code>import polars as pl\npd.Series(\nlist(pl.DataFrame(df).with_column(\npl.fold(0, lambda acc, s: acc + s, pl.all()).alias(\"sum\")\n)[:,-1])\n)\n</code></pre> Time to apply with polars: <code>1.012 seconds</code></p> <p>Both function return a <code>pandas.Series</code> object, however, Polars is 3.17x faster than pandas at <code>apply</code>. In a near future, if time complexity becomes an issue, we might turn the entire code base to polars.</p>"},{"location":"getting_started/main_concepts/#step-by-step-example","title":"Step-by-Step Example","text":"<p>Let's design the following strategy:</p> <ul> <li>Long-Short Equity</li> <li>Top-Down (from sector to stock level)</li> <li>Risk optimized (8% target volatility)</li> <li>Market neutral</li> <li>Sentiment-based, which integrates both sector momentum reversion and trend following techniques.</li> </ul>"},{"location":"getting_started/main_concepts/#step-1-blocks","title":"Step 1: Blocks","text":"<p>To calculate scores, we follow a set of predefined rules that compute the deviation between long-term and short-term sector rankings. This generates a composite factor that averages the relative price return momentum within each sector. These techniques can be found in the sentiment folder under the <code>Reversion</code> and <code>TrendFollowing</code> alpha blocks. For more information about these techniques and other blocks, please visit the Model Glossary.</p> <p>The Strategy class is initialized by defining the steps as outlined above and setting the <code>topdown</code> parameter to <code>True</code>. </p> <pre><code>from opendesk import Strategy\nfrom opendesk.blocks import Reversion, TrendFollowing\nstrategy = Strategy(\nsteps = [\n(\"reversion\", Reversion),\n(\"trend_following\", TrendFollowing)\n],\ntopdown=True, \nmapping_table=mapping_table\n)\n</code></pre> <p>Mapping Table</p> <p>Because we are implementing a top-down strategy (parameter <code>topdown=True</code>), the <code>mapping_table</code> is also passed. The mapping table is a dictionnary, which associates stocks with sectors, as follow:</p> <p> <pre><code>$ pd.Series(mapping_table)\n&lt;span style=\"color: grey;\"&gt;stock 1       sector 1\nstock 2       sector 2\nstock 3       sector 3\nstock 4       sector 4\nstock 5       sector 5\n                ...  \nstock 96      sector 6\nstock 97      sector 7\nstock 98      sector 8\nstock 99      sector 9\nstock 100     sector 10\nLength: 100, dtype: object\n&lt;/span&gt;\n</code></pre> </p>"},{"location":"getting_started/main_concepts/#step-2-exposures","title":"Step 2: Exposures","text":"<p>The returns time-series (at the sector level) are fit using the <code>fit()</code> method, and the portfolio exposures (tilts) are estimated through <code>estimate()</code> by summing the individual scores:</p> <pre><code>$ strategy.fit(model_data).estimate(sum)\n&lt;span style=\"color: yellow;\"&gt;[Parallel(n_jobs=-1)]:&lt;/span&gt;   &lt;span style=\"color: lime;\"&gt;1 tasks&lt;/span&gt;\n&lt;span style=\"color: yellow;\"&gt;[Parallel(n_jobs=-1)]:&lt;/span&gt;   &lt;span style=\"color: lime;\"&gt;Done 2 out of 2 | elapsed: 0.5s&lt;/span&gt;\n&lt;span style=\"color: yellow;\"&gt;[Parallel(n_jobs=-1)]:&lt;/span&gt;   &lt;span style=\"color: lime;\"&gt;Done 2 out of 2 | elapsed: 0.5s finished&lt;/span&gt;\n</code></pre> <p>A summary breakdown and final tilts can be shown using the <code>breakdown</code> and <code>exposure</code> attributes, respectively.:</p> Breakdown Exposures <p> <pre><code>$ strategy.breakdown\n&lt;span style=\"color: grey;\"&gt;           Trendfollowing  Reversion\nsector 1                1          1\nsector 2                0          1\nsector 3                0          0\nsector 4               -2          0\nsector 5               -1         -1\nsector 6                2         -2\nsector 7                2         -1\nsector 8               -2          0\nsector 9               -1          0\nsector 10               1          2\nName: sum, dtype: int64 \n&lt;/span&gt;\n</code></pre> </p> <p> <pre><code>$ strategy.exposures\n&lt;span style=\"color: grey;\"&gt;sector 1     2\nsector 2     1\nsector 3     0\nsector 4    -2\nsector 5    -2\nsector 6     0\nsector 7     1\nsector 8    -2\nsector 9    -1\nsector 10    3\nName: sum, dtype: int64\n&lt;/span&gt;\n</code></pre> </p>"},{"location":"getting_started/main_concepts/#step-3-portfolio-construction","title":"Step 3: Portfolio Construction","text":"<p>To find individual stocks weights, you can use the <code>optimize()</code> method, which allows us to align the portfolio with your objectives and constraints. </p> <p>The modular structure of our code pipeline enables you to choose various optimization techniques, return estimation methods, and risk models within the <code>portfolio()</code> method. </p> <p>Additionally, the built-in functionality  incorporates custom objetives and constraints, such as lower and upper bounds for each individual stocks. You can adjust the weightings of you investment strategy in order to achieve a targeted level of volatility and market neutrality, as follow:</p> <pre><code>output = strategy.portfolio(stock_prices).optimize( # (1)\nexpected_returns=\"capm_return\", # (2)\noptimizer=\"efficient_semivariance\",  \ntarget=\"efficient_return\",\ntarget_return=0.01\nweight_bounds=(-1, 1) # (3)\nconstraints=[\nlambda w: w &lt;=  0.1, \nlambda w: w &gt;= -0.1\n]\n)\n</code></pre> <ol> <li>Because it is a top-down strategy, we want to optimize your portfolio (finding weights which align with both objectives and constraints) at the stock level.</li> <li>Mean-variance optimization requires knowledge of the expected returns.</li> <li>Minimum and maximum weight of each asset. When <code>weight_bounds=(-1, 1)</code>, it allows for portfolios with shorting.</li> </ol> <p>Which returns an <code>OrderedDict</code> of <code>clean_weights</code>, a utility function provided by the PyPortfolioOpt library and integrated within the module. This function allows us to filter out any weightings with absolute values below a specified cutoff, setting them to zero, and rounding the remaining entities.</p> <pre><code>$ weights = pd.Series(output, name=\"weights\")\n&lt;span style=\"color: grey;\"&gt;asset 1     -0.09\nstock 2      0.09\nstock 3     -0.08\nstock 4      0.00\nstock 5      0.00\nstock 96     0.06\nstock 97    -0.04\nstock 98     0.00\nstock 99     0.00\nstock 100    0.00\nName: weights, Length: 100, dtype: float64\n&lt;/span&gt;\n</code></pre> <ol> <li> <p>Kowalski, M. (1979). Algorithm = logic + control. Communications of the ACM, 22(7), 424-436.\u00a0\u21a9</p> </li> <li> <p>Bass, L., Clements, P., &amp; Kazman, R. (2003). Software architecture in practice (2nd ed.). Boston, MA: Addison-Wesley.\u00a0\u21a9</p> </li> <li> <p>Rouwenhorst, K. G., &amp; Wu, G. S. (2001). Modular investment strategies. Journal of Financial Economics, 61(3), 369-403.\u00a0\u21a9</p> </li> <li> <p>Faber, M., &amp; O'Shaughnessy, J. (2013). Building investment strategies with building blocks. Journal of Portfolio Management, 39(4), 104-115.\u00a0\u21a9</p> </li> <li> <p>By default joblib.Parallel uses the 'loky' backend module to start separate Python worker processes to execute tasks concurrently on separate CPUs. More information available in the Joblib Documentation.\u00a0\u21a9</p> </li> </ol>"},{"location":"getting_started/optimize_blocks/","title":"Optimize Blocks","text":"<p>Portfolio optimization is the process of selecting the optimal mix of assets in a portfolio in order to maximize returns while minimizing risk. One method of portfolio optimization involves the use of alpha scores. By selecting assets with high alpha scores, an investor can potentially achieve higher probability of positive returns while taking on the same level of risk. The optimal portfolio is typically determined through the use of statistical analysis and optimization techniques, such as mean-variance or Black-Litterman optimization.</p> <p>Portfolio optimization using alpha scores has been studied extensively in the financial literature. For example, in a study published in the Journal of Financial Economics1, Chen, Novy-Marx, and Zhang (2013) found that portfolios constructed using alpha scores significantly outperformed traditional market-cap weighted portfolios. Another study by Geczy, Musto, and Reed (2005) published in the Review of Financial Studies2 found that portfolios constructed using alpha scores had higher Sharpe ratios (a measure of risk-adjusted returns) compared to traditional portfolios.</p> <p>Black-Litterman Approach</p> <p>This method is also particularly useful in the Black-Litterman  (BL) model, which is a Bayesian approach to asset allocation. The BL model combines an initial estimate of returns with specific views on certain assets to generate a revised estimate of expected returns. This revised estimate, known as the posterior estimate, is then used to optimize the allocation of assets in accordance with a predetermined set of objectives (e.g., maximizing Sharpe ratio) and constraints.</p> <p>Risk Considerations</p> <p>It's important to note that while alpha scores can be useful for portfolio optimization, they are not a guarantee of success. Like any investment strategy, portfolio optimization using alpha scores carries its own set of risks and uncertainties. It is always important for investors to carefully consider their investment goals and risk tolerance before making any investment decisions.</p>"},{"location":"getting_started/optimize_blocks/#key-takeaways","title":"Key Takeaways","text":"<p>An interval query is used to assign scores to elements in a portfolio. These scores are then used to determine the relative weightings of the highest and lowest elements in the portfolio. The resulting portfolio reflects these weightings and adheres to any specified objectives and constraints.</p> <pre><code>flowchart TD\n    B[Strategy] --&gt; E[Exposures];\n    E -- Default Config --&gt; M[Map]\n    E -- Custom Config --&gt; M\n    M -- Bound Constraints --&gt; P[Portfolio]\n    P -- Objectives --&gt; O[Optimizer]\n    P -- Constraints --&gt; O\n    O -- Weights --&gt; P\n    P -. Analysis .-&gt; Backtests</code></pre> <p>Why Are We Not Creating Weights in the First Place?</p> <p>There are a few reasons why you might calculate scores rather than directly assigning \"weights\" to potential assets in their investment portfolios. </p> <p>One reason is that scores can be used to rank potential investments relative to one another, while weights are typically used to indicate the proportion of your total portfolio that should be allocated to a particular asset. This means that scores can be used to identify which assets are the most attractive candidates for inclusion in a portfolio, while weights are used to determine how much of an your capital should be allocated to each asset. </p> <p>Another reason is that scores can be based on a variety of different factors and criteria, whereas weights are usually based on a single factor (e.g., expected return or risk). By using scores, you can take a more holistic view of potential investments and consider multiple factors when making decisions. </p> <p>Furthermore, the allocation of weights in the portfolio would not consider the overall goals and limitations of the portfolio. It would only be based on the weights where alpha scores have been calculated, such as at the sector level.</p>"},{"location":"getting_started/optimize_blocks/#step-by-step-example","title":"Step-by-Step Example","text":"<p>Portfolio construction, which involves optimizing the allocation of assets within a portfolio, can be a complex and nuanced process. We have developed a method that allows for greater flexibility and experimentation in the portfolio optimization process. This approach enables the exploration of a wide range of potential portfolio compositions, and the example provided illustrates this method applied from the initial stages of portfolio construction.</p> <pre><code>from opendesk import Strategy\nfrom opendesk.blocks import Reversion\nstrategy = Strategy([(\"reversion\", Reversion)]).fit(df).estimate(sum) # (1)\n</code></pre> <ol> <li>Calculate sentiment using Reversion Ranking Method.     More information provided in the Model Glossary.</li> </ol>"},{"location":"getting_started/optimize_blocks/#step-1-portfolio","title":"Step 1: Portfolio","text":"<p>We aim to find weights for a large universe of stocks:</p> <pre><code>strategy.portfolio(data=stock_prices) # (1)\n</code></pre> <ol> <li>pandas.DataFrame object, with specifiy the variation of stock prices over time.</li> </ol>"},{"location":"getting_started/optimize_blocks/#step-2-optimize","title":"Step 2: Optimize","text":"<p>The wrapper class inherite from <code>PortfolioConstruction</code>, which adds the <code>optimize</code> public method to your toolbox, allowing for the efficient computation of optimized asset weights. Constraints are lambda functions (e.i. all assets must be lower or equal to 10% of the total portfolio would simply translate to <code>[lambda w: w &lt;= .1]</code>. This constraint must satisfy DCP rules, i.e be either a linear equality constraint or convex inequality constraint. Here is an example with \"min_volatility\", which finds the minimum risk portfolio:</p> <pre><code>weights = strategy.optimize(  \nweight_bounds=(-1, 1), # (1)\ntarget=\"min_volatility\",\nconstraints=[lambda w: w &lt;= .1]\n)\n</code></pre> <ol> <li><code>weight_bounds</code> parameter serves as a constraint by limiting the minimum and maximum weight of each asset in portfolios. Because it ranges from <code>-1</code> to <code>1</code>, it allows Long and Shorts.</li> </ol> <pre><code>$ pd.Series(weights, name=\"weights\")\n&lt;span style=\"color: grey;\"&gt;asset 1      0.10\nasset 2      0.03\nasset 3     -0.02\nasset 4      0.03\nasset 5     -0.05\nasset 96     0.00\nasset 97    -0.09\nasset 98     0.00\nasset 99    -0.07\nasset 100    0.00\nName: weights, Length: 100, dtype: float64\n&lt;/span&gt;\n</code></pre> <ol> <li> <p>Chen, L., Novy-Marx, R., &amp; Zhang, L. (2013). Factor Premia and Interaction with the Market Portfolio. Journal of Financial Economics, 110(1), 1-35.\u00a0\u21a9</p> </li> <li> <p>Geczy, C., Musto, D., &amp; Reed, A. (2005). A simple approach to performance attribution for hedge funds: the case of equity market neutral strategies. Review of Financial Studies, 18(2), 367-384.\u00a0\u21a9</p> </li> </ol>"},{"location":"model_glossary/","title":"Model Glossary","text":""},{"location":"model_glossary/#coming-soon","title":"Coming Soon!","text":""},{"location":"model_glossary/phase_identification/","title":"Phase Identification","text":"<p>While academics have focused on type I (false positive) and type II errors  (false negative) for investment strategy development, they have failed to  analyse changing market conditions. Investors are interested less in knowing the  statistical significance of their strategies, than in knowing whether their  strategy will work in the future. Whether it will work in the future is dependent  on the durability of market conditions which existed while developing it.</p> <p>Marcos Lopez de Prado, 2019.</p> <p>Phase identification enables investors to more accurately assess economic  regimes through time, beyond the ongoing switch between market risk-on and off.  It makes the point of new paradigm for both quant and fundamental PMs, in order  to identify a cause-effect mechanism to develop a thematic and capturing  asymmetrical distribution:</p> <ul> <li>Avoid all-regime strategies (Marcos Lopez de Prado, 2019)</li> <li>Allow regime-specific mean and standard deviation estimates</li> </ul> <p>Maximising returns means applying the below dynamic investment approach  associated with a cycle:</p> <ul> <li>Contrarian: When the rate of change is null (peak/trough)<ul> <li>Long risky assets as they fall</li> <li>Short risky assets as they rise</li> </ul> </li> <li>Trend-following otherwise (momentum - crowded investment styles)<ul> <li>Long risky assets as they rise</li> <li>Short risky assets as they fall</li> </ul> </li> </ul> <p>Understanding the balance between the size of the two groups is essential for  market behaviour:</p> <ul> <li>When contrarian strategies succeed for a while, they attract followers and  markets may become too stable (slow to adjust to economic developments)</li> <li>Conversely, when momentum strategies succeed for a while, they attract new  followers and markets may become too volatile (as such overreacting to news flows)</li> </ul> <p>Excess popularity of one approach eventually results in excess gains for the  opposite style, a process which over time should drive toward a balance. </p> <p>Long-Term Returns</p> <p>Therefore, long term returns should be positive (negative) only if the  overall average cycle trend is upward (downward) sloping. We call this  phenomenon Growth or wealth creation (destruction).</p>"},{"location":"model_glossary/phase_identification/factor_performance/","title":"\ud83d\udd12 Factor Performance","text":"<p>Used by practitioners</p> <p>Similar strategy presented by State Street SPDR Americas Research, Sector  Business Cycle Analysis and Orion Capital Partners1</p> <p>The scientific method is an empirical method of acquiring knowledge that has  characterized the development of science. It involves observation and skepticism,  given that cognitive assumptions can distort how one interprets the observation.  We first tend to identify market and economic regimes with both data  input and rational methods. We then analyse market performance, through the  lens of the below groupings:</p> <ul> <li>GICS GICS - Global Industry Classification Standard</li> <li>Risk factors Factor Performance in Bull and Bear Markets or Factor\u2019s Performance During Various Market Cycles, both from QuantPedia</li> <li>Network graph, based on correlation or factor similarity (e.g. factor contribution, dominant factor polymodels)</li> </ul> <ol> <li> <p>G. Monarcha, A systematic macro-based approach to sector rotation, Head  of Research, Orion Financial Partners, June 2022 update (First version: September 2021))\u00a0\u21a9</p> </li> </ol>"},{"location":"model_glossary/phase_identification/markov_models/","title":"\ud83d\udd12 Markov Models","text":""},{"location":"model_glossary/phase_identification/markov_models/#docs-in-progress","title":"DOCS IN PROGRESS","text":""},{"location":"model_glossary/phase_identification/signal_models/","title":"\ud83d\udd12 Signal Models","text":""},{"location":"model_glossary/phase_identification/signal_models/#introduction","title":"Introduction","text":"<p>In order to guide asset allocation decisions and allocate to industries at the optimal time, we believe the growth cycle plays an important role. We combine top-down insights and expert bottom-up fundamental analysis to understand the dynamics and drivers that determine industry performance within the cycle. In addition, the duration of a traditional growth cycle is considered in terms of years and is often referenced in relation to the progression of credit growth and capital expenditure in a particular economy.</p>"},{"location":"model_glossary/phase_identification/signal_models/#economic-cycles","title":"Economic cycles","text":""},{"location":"model_glossary/phase_identification/signal_models/#classical-business-cycle","title":"Classical business cycle","text":"<p>Business cycles are a type of fluctuation found in the aggregate economic activity of nations that organize their work mainly in business enterprises: a cycle consists of expansions occurring at about the same time in many economic activities, followed by similarly general recessions, contractions, and revivals which merge into the expansion phase of the next cycle.</p> <p>Burns and Mitchell, 1946</p> <p>The business cycle is meant to reproduce the cycle of the global level of activity of a country. The turning points of that cycle:</p> <ul> <li>Peak \u201cB\u201d</li> <li>Trough \u201cC\u201d</li> </ul> <p>are separate periods of recessions from periods of expansions.</p> <pre><code>flowchart LR\n  B --&gt; C;\n  C --&gt; B;</code></pre> <p>Burns and Mitchell (1946) point out two main stylised facts of the economic cycle:</p> <ol> <li>Correlation among individual economic variables: Most of macroeconomic time series evolve together along the cycle</li> <li>Non-linearity: The effect of a shock depends on the rest of the economic environment. In other words, economic dynamics during economically stressful times are potentially different from normal times. For instance, small shock, such as a decrease in housing prices, can sometimes have large effects, such as recessions</li> </ol>"},{"location":"model_glossary/phase_identification/signal_models/#the-growth-cycle","title":"The growth cycle","text":"<p>Definition introduced by Mintz (1974):</p> <ul> <li>Fluctuations of the GDP around its long-term trend</li> <li>Absolute prolonged declines in the level of economic activity tend to be rare events, so that in practice many economies do not very often exhibit recessions in classical terms</li> </ul> <p>Growth cycle turning points have a clear meaning: </p> <ul> <li>Peak \u201cA\u201d is reached when the growth rate decreases below the trend growth rate </li> <li>Trough \u201cD\u201d is reached when the growth rate overpasses it again</li> </ul> <pre><code>flowchart LR\n  A --&gt; D;\n  D --&gt; A;</code></pre> <p>Those downward and upward phases are respectively named slowdown and acceleration.</p> <p>Long-Term Trend</p> <p>If the long-term trend is considered as the estimated potential level (the potential output is the maximum amount of goods and services an economy can turn out at full capacity), then the growth cycle equals the output gap. </p> <p>A turning point of the output gap occurs when the current growth rate of the activity is above or below the potential growth rate, thereby signalling increasing or decreasing inflation pressures.</p>"},{"location":"model_glossary/phase_identification/signal_models/#the-abcd-approach","title":"The ABCD approach","text":"<p>Traditional growth cycle models tend to assume a linear progression through four discrete phases within the growth cycle: </p> <ul> <li>Recovery</li> <li>Expansion</li> <li>Slowdown</li> <li>Contraction</li> </ul> <p>The ABCD approach (Anas and Ferrara, 2004) refines the description of different economic phases by jointly considering the classical business cycle and the growth cycle:</p> <ul> <li>Let us suppose that the current growth rate of the activity is above the trend growth rate (acceleration phase)</li> <li>Point \u201cA\u201d: The downward movement will first materialize when the growth rate will decrease below the trend growth rate</li> <li>Point \u201cB\u201d: If the slowdown gains in intensity, the growth rate could become negative enough to provoke a recession</li> <li>Point \u201cC\u201d: Eventually, the economy should start to recover and exits from the recession</li> <li>Point \u201cD\u201d: As the recovery strengthens, the growth rate should overpass its trend. However, a slowdown will not automatically translate into a recession: if the slowdown is not severe enough to become a recession, then Point \u201cA\u201d will not be followed by Point \u201cB\u201d, but by Point \u201cD\u201d.</li> </ul> <pre><code>flowchart LR\n  A --- B &amp; D --- C;</code></pre> <p>Note on Cycles</p> <p>This framework improves thus the classical analysis of economic cycles by allowing sometimes two distinct phases, if the slowdown is not severe enough to become a recession, and sometimes four distinct phases, if the growth rate of the economy becomes negative enough to provoke a recession. In other words, all recessions involve slowdowns, but not all slowdowns involve recessions.</p>"},{"location":"model_glossary/phase_identification/signal_models/#the-non-linear-approach","title":"The non-linear approach","text":"<p>An alternative construct, where a growth cycle consists of six phases and progresses in a non-linear fashion, with each phase playing out over approximately four-to-six months. A six-phase model can, in our view, better explain the growth narrative as well as offer a consistent framework to understanding industry returns across emerging markets equities</p> <p>The six-phase growth model \u2014 constructed using a large set of high frequency macroeconomic indicators from different different economies \u2014 tracks the transition across the stages of the cycle in a more consistent manner when compared to traditional models.</p> <pre><code>flowchart LR\n  AD -.-&gt; A --- B &amp; D --- C;\n  C -.-&gt; BC;\n  BC -.-&gt; C;\n  A -.-&gt; AD;</code></pre> <p>For example, the growth model can depict the transition from the recovery phase to the expansion phase of the cycle, while also accounting for the alternative scenario that can occur when a recovery fails to take hold \u2014 as it repeatedly did, for example, in emerging markets between 2011 and 2015. Similarly, a slowdown phase does not always transition into a contraction phase per the traditional model. Our growth model accounts for periods when a slowdown is followed by a re-acceleration in growth before slowing again, as emerging markets experienced between 2004 and 2007.</p>"},{"location":"model_glossary/phase_identification/cluster_models/","title":"\ud83d\udd12 Cluster Models","text":""},{"location":"model_glossary/phase_identification/cluster_models/#time-series-classification","title":"Time-series classification","text":"<p>Time-series classification is a general task that can be useful to identify  regime change or predefined groups, using labeled training data.</p> <p>Multi-label classification is a process of categorizing a given set of time interval data into multiple classes. It requires the machine to learn how to  assign a class label to examples from the problem domain.</p> <p>Multi-label classification is a supervised machine learning module that is used  for classifying time interval data into groups. The goal is to predict the  categorical class labels. A vast body of research could be implemented, on top  of pre-processing features that prepare the data for modeling. In python,  Pycaret  has over 18 ready-to-use algorithms and several plots to analyze the  performance of trained models (e.g. ROC curve, confusion matrix).</p>"},{"location":"model_glossary/phase_identification/cluster_models/asset_allocation/","title":"\ud83d\udd12 Asset Allocation","text":""},{"location":"model_glossary/phase_identification/cluster_models/asset_allocation/#docs-in-progress","title":"DOCS IN PROGRESS","text":""},{"location":"model_glossary/phase_identification/cluster_models/gaussian_mixture/","title":"\ud83d\udd12 Gaussian Mixture","text":""},{"location":"model_glossary/phase_identification/cluster_models/gaussian_mixture/#docs-in-progress","title":"DOCS IN PROGRESS","text":""},{"location":"model_glossary/sentiment/hurst_exponent/","title":"\ud83d\udd12 Hurst Exponent","text":""},{"location":"model_glossary/sentiment/hurst_exponent/#docs-in-progress","title":"DOCS IN PROGRESS","text":""},{"location":"model_glossary/sentiment/range_strategy_models/","title":"\ud83d\udd12 Range Strategy","text":""},{"location":"model_glossary/sentiment/range_strategy_models/#docs-in-progress","title":"DOCS IN PROGRESS","text":""},{"location":"model_glossary/sentiment/reversion_models/","title":"\ud83d\udd12 Reversion Models","text":""},{"location":"model_glossary/sentiment/reversion_models/#docs-in-progress","title":"DOCS IN PROGRESS","text":""},{"location":"model_glossary/sentiment/short_interest_models/","title":"\ud83d\udd12 Short-Interest Models","text":""},{"location":"model_glossary/sentiment/short_interest_models/#docs-in-progress","title":"DOCS IN PROGRESS","text":""},{"location":"model_glossary/sentiment/trend_following_models/","title":"\ud83d\udd12 Trend Following Models","text":""},{"location":"model_glossary/sentiment/trend_following_models/#docs-in-progress","title":"DOCS IN PROGRESS","text":""},{"location":"model_glossary/valuation/zscore_models/","title":"\ud83d\udd12 Zscore Models","text":""},{"location":"model_glossary/valuation/zscore_models/#docs-in-progress","title":"DOCS IN PROGRESS","text":""},{"location":"terminal/","title":"Terminal","text":""},{"location":"terminal/#coming-soon","title":"Coming Soon!","text":""}]}